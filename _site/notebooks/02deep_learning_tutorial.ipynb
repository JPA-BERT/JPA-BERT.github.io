{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "2020-0722deep_learning_tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JPA-BERT/jpa-bert.github.io/blob/master/notebooks/02deep_learning_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbHwULUsSUxh",
        "colab_type": "text"
      },
      "source": [
        "# PyTorch tutorial の内容について(2)\n",
        "\n",
        "- date: 2020-0722\n",
        "- author: 浅川伸一\n",
        "\n",
        "[https://github.com/pytorch/tutorials/tree/master/beginner_source/nlp](https://github.com/pytorch/tutorials/tree/master/beginner_source/nlp) を見ると\n",
        "PyTorch で 自然言語処理を行う場合のチュートリアルは以下とおりである\n",
        "\n",
        "# Deep Learning for NLP with Pytorch\n",
        "\n",
        "1. [pytorch_tutorial.py](https://github.com/pytorch/tutorials/blob/master/beginner_source/nlp/pytorch_tutorial.py): \n",
        "\t[PyTorch 入門 Introduction to PyTorch](https://pytorch.org/tutorials/beginner/nlp/pytorch_tutorial.html)\n",
        "\n",
        "2. [deep_learning_tutorial.py](https://github.com/pytorch/tutorials/blob/master/beginner_source/nlp/deep_learning_tutorial.py): \n",
        "\t[PyTorch による深層学習 Deep Learning with PyTorch](https://pytorch.org/tutorials/beginner/nlp/deep_learning_tutorial.html)\n",
        "\n",
        "3. [word_embeddings_tutorial.py](https://github.com/pytorch/tutorials/blob/master/beginner_source/nlp/word_embeddings_tutorial.py): \n",
        "\t[単語埋め込み:語彙的意味の符号化 Word Embeddings: Encoding Lexical Semantics](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html)\n",
        "\n",
        "4. [sequence_models_tutorial.py]((https://github.com/pytorch/tutorials/blob/master/beginner_source/nlp/sequence_models_tutorial.py): \n",
        "\t[系列モデルと LSTM Sequence Models and Long-Short Term Memory Networks](https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html)\n",
        "\n",
        "5. [advanced_tutorial.py]((https://github.com/pytorch/tutorials/blob/master/beginner_source/nlp/advanced_tutorial.py): \n",
        "\t[動的意思決定と双方向 LSTM 条件付き確率場 Advanced: Making Dynamic Decisions and the Bi-LSTM CRF](https://pytorch.org/tutorials/beginner/nlp/advanced_tutorial.html)\n",
        "\n",
        "\n",
        "以下では，このうちの 2 について解説している。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gN7qtemWSMMD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wh6QE7BdSMMH",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "PyTorch によるディープラーニング\n",
        "********************************\n",
        "\n",
        "## ディープラーニングのブルディンクブロック: アフィン変換，非線形性，目的関数\n",
        "\n",
        "ディープラーニングは、線形性と非線形性を巧妙な方法で組み合わせることで構成されています。\n",
        "非線形性を導入することで、強力なモデルを作ることができます。\n",
        "このセクションでは、これらのコアコンポーネントで遊び、目的関数を構成し、モデルがどのように訓練されるかを見ていきます。\n",
        "\n",
        "<!--\n",
        "Deep learning consists of composing linearities with non-linearities in clever ways. \n",
        "The introduction of non-linearities allows for powerful models. \n",
        "In this section, we will play with these core components, make up an objective function, and see how the model is trained.\n",
        "-->\n",
        "\n",
        "### アフィン写像 <!--Affine Maps-->\n",
        "\n",
        "ディープラーニングの中核を成すものの一つがアフィン写像であり、次式の関数 $f(x)$ で表します。\n",
        "\n",
        "<!--One of the core workhorses of deep learning is the affine map, which is a function $f(x)$ where-->\n",
        "\n",
        "\\begin{align}\n",
        "f(x) = Ax + b\n",
        "\\end{align}\n",
        "\n",
        "行列 $A$ とベクトル $x, b$ を求めます。ここで学習するパラメータは $A$ と $b$ です。\n",
        "$b$ はバイアスとして使用されます。\n",
        "\n",
        "PyTorch や他のほとんどのディープラーニングフレームワークでは 従来の線形代数とは少し違うことをしています。\n",
        "それは、入力の列の代わりに 行を写像します。\n",
        "つまり 以下の出力の $i$'番目の行は $A$ の下の入力の $i$'th 行にバイアス項を加えたものです。\n",
        "下の例を見てください。\n",
        "\n",
        "\n",
        "<!--\n",
        "for a matrix $A$ and vectors $x, b$. The parameters to be learned here are $A$ and $b$. Often, $b$ is refered to\n",
        "as the *bias* term.\n",
        "\n",
        "PyTorch and most other deep learning frameworks do things a little differently than traditional linear algebra. \n",
        "It maps the rows of the input instead of the columns. \n",
        "That is, the $i$'th row of the output below is the mapping of the $i$'th row of the input under $A$, plus the bias term. Look at the example below.\n",
        "-->\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGpQj5J6SMMH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "daeedc66-74c2-4df6-e439-15f51c73c7f9"
      },
      "source": [
        "# Author: Robert Guthrie\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.manual_seed(1)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fa696ff0120>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ufx9cw8OSMMJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "35dd2e39-c7bd-4884-a690-7061ccb297a5"
      },
      "source": [
        "lin = nn.Linear(5, 3)  # 5 次のベクトルから 3 次のベクトルへの線形写像 ``nn.Linear`` #  maps from R^5 to R^3, parameters A, b\n",
        "# データは 2 行 5 列 すなわち 5 次元のベクトルからなるデータが 2 つ # data is 2x5.  A maps from 5 to 3... can we map \"data\" under A?\n",
        "data = torch.randn(2, 5)\n",
        "print(lin(data))  # 実際には 行列積 "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.1755, -0.3268, -0.5069],\n",
            "        [-0.6602,  0.2260,  0.1089]], grad_fn=<AddmmBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TdkNb63SMMM",
        "colab_type": "text"
      },
      "source": [
        "### 非線形性 \n",
        "\n",
        "次の事実に注意してください。2 つのアフィン写像 $f(x) = Ax + b$ と $g(x) = Cx + d$ があるとします。\n",
        "$f(g(x))$ は何でしょうか？\n",
        "\n",
        "<!--First, note the following fact, which will explain why we need non-linearities in the first place. Suppose we have two affine maps $f(x) = Ax + b$ and $g(x) = Cx + d$. \n",
        "What is $f(g(x))$?\n",
        "-->\n",
        "\n",
        "\\begin{align}f(g(x)) = A(Cx + d) + b = ACx + (Ad + b)\\end{align}\n",
        "\n",
        "$A$ と $C$ は行列であり，$Ad + b$ はベクトルです。\n",
        "アフィン写像の合成はアフィン写像であることがわかります。\n",
        "\n",
        "<!--so we see that composing affine maps gives you an affine map.-->\n",
        "\n",
        "<!--\n",
        "From this, you can see that if you wanted your neural network to be long chains of affine compositions, that this adds no new power to your model than just doing a single affine map.\n",
        "-->\n",
        "\n",
        "このことから，ニューラルネットワークをアフィン写像の幾重にも重ねた，ディープニューラルネットワークモデル\n",
        "を作成する場合，\n",
        "単一のアフィン写像による成績以上の性能は得られないことを意味します。\n",
        "\n",
        "<!--\n",
        "If we introduce non-linearities in between the affine layers, this is no longer the case, and we can build much more powerful models.\n",
        "-->\n",
        "\n",
        "逆に，アフィン写像の間に非線形性を導入するとにより，より強力なモデルを構築することができます。\n",
        "\n",
        "<!--There are a few core non-linearities.-->\n",
        "いくつかのコアとなる非線形性があります。\n",
        "\b$\\tanh(x)$, $\\sigma(x)$ (シグモイド関数), $\\text{ReLU}(x)$ が一般的です。\n",
        "他にも数多非線形変換がありうるのに，なぜこれらが用いられるのか疑問に思うかも知れません。\n",
        "理由は勾配計算が容易だからです。勾配の計算はニューラルネットワークの学習には必須です。\n",
        "\n",
        "<!--$\\tanh(x), \\sigma(x), \\text{ReLU}(x)$ are the most common. \n",
        "You are probably wondering: \"why these functions? I can think of plenty of other non-linearities.\" \n",
        "The reason for this is that they have gradients that are easy to compute, and computing gradients is essential for learning.-->\n",
        "\n",
        "<!--For example-->\n",
        "例えば\n",
        "\n",
        "\\begin{align}\\frac{d\\sigma}{dx} = \\sigma(x)(1 - \\sigma(x))\\end{align}\n",
        "\n",
        "<!--A quick note: although you may have learned some neural networks in your intro to AI class where $\\sigma(x)$ was the default non-linearity, typically people shy away from it in practice. \n",
        "This is because the gradient *vanishes* very quickly as the absolute value of the argument grows. \n",
        "Small gradients means it is hard to learn. Most people default to tanh or ReLU.\n",
        "-->\n",
        "\n",
        "昔からニューラルネットワークをご存知の方は，シグモイド関数 $\\sigma(x)$ がデフォルトであると思われていることでしょう。ですが，最近ではシグモイド関数は用いられません。理由は勾配消失問題に対処するのが難しいからです。\n",
        "このため，現在では $\\tanh(x)$ や ReLU (整流線型ユニット) が用いられます。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h80HlbJKSMMM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "9fbbcd1f-5e44-4ee7-915c-1088e74a1495"
      },
      "source": [
        "# pytorch では ほとんどの非線形活性化関数は torch.functional で定義されています (F としてインポートしています)\n",
        "# 非線形型は一般的にアフィン写像のような パラメータを持たちません。\n",
        "# すなわち 学習中に更新されるパラメータを持ちません。\n",
        "data = torch.randn(2, 2)\n",
        "print(data)\n",
        "print(F.relu(data))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.5404, -2.2102],\n",
            "        [ 2.1130, -0.0040]])\n",
            "tensor([[0.0000, 0.0000],\n",
            "        [2.1130, 0.0000]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JwkxjtQSMMO",
        "colab_type": "text"
      },
      "source": [
        "### ソフトマックスと確率\n",
        "\n",
        "<!--The function $\\text{Softmax}(x)$ is also just a non-linearity, but it is special in that it usually is the last operation done in a network. \n",
        "This is because it takes in a vector of real numbers and returns a probability distribution. Its definition is as follows. \n",
        "Let $x$ be a vector of real numbers (positive, negative, whatever, there are no constraints). Then the i'th component of $\\text{Softmax}(x)$ is-->\n",
        "\n",
        "関数 $\\text{Softmax}(x)$ も単なる非線形活性化関数の一つです。\n",
        "ですが，通常ネットワーク内で最終層で行われる操作であるという点で特殊です。\n",
        "ソフトマックス関数 は実数のベクトルを引数として取り，\n",
        "各ニューロン (ユニット) の確率分布を返すからです。\n",
        "ソフトマックス関数の定義は次のようになります。\n",
        "$x$ を実ベクトルとする (正でも負でも何でもよく，制約はない)。\n",
        "$text{Softmax}(x)$ の i 番目の成分は以下になります:\n",
        "\n",
        "\\begin{align}\\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}\\end{align}\n",
        "\n",
        "<!--It should be clear that the output is a probability distribution: \n",
        "each element is non-negative and the sum over all components is 1.\n",
        "\n",
        "You could also think of it as just applying an element-wise exponentiation operator to the input to make everything non-negative and then dividing by the normalization constant.\n",
        "-->\n",
        "\n",
        "出力値が確率分布をなす (各要素は非負で，かつ，全要素の総和が 1 ) ことは，明らかに重要です。\n",
        "入力に要素ごとの 指数化演算子を適用してすべてを非負にして，、正規化定数で除算すると考えることもできます。\n",
        "\n",
        "---\n",
        "\n",
        "### 訳注: \n",
        "Python で頻用される科学計算ライブラリ `scipy.special` には `scipy.special.logsumexp()` という関数が用意されている。\n",
        "上記ソフトマックス関数を計算する際に，分母 $\\sum\\exp(x_j)$ は 指数変換した各値を総和する (`sumexp` する) ことを意味し，\n",
        "確率密度関数の対数尤度を計算する場合には，この `sumexp` を対数変換するので `logsumexp` と命名された。\n",
        "`logsumexp()` 関数を用いてソフトマックス関数を定義すれば以下のようになる:\n",
        "\n",
        "\\begin{align}\\log\\text{softmax}(x_i) = x_i - \\text{logsumexp(x)}\\end{align}\n",
        "\n",
        "すなわちソフトマックス関数の対数を計算するときには，各値から `logsumexp(x)` を減じるだけになるので，\n",
        "計算が極端にかんたんになる。\n",
        "筆者の知る限り，Fortran 時代から logsumexp ルーチンは存在する。もはやその起源や命名者を特定できないように見受けられる。\n",
        "実際，以下のコードは同じ値を与える。\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "from scipy.special import softmax, logsumexp\n",
        "\n",
        "x1, x2, x3 = 1., 2., 3. \n",
        "print(np.log(softmax([x1, x2, x3]))[0])\n",
        "print( x1 - logsumexp([x1, x2, x3]))\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uN01YI8lSMMR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "ee7ad632-eec6-4db4-9ffb-62f3f7157ea8"
      },
      "source": [
        "# ソフトマックス関数 Softmax は torch.nn.functional でも定義されている\n",
        "data = torch.randn(5)\n",
        "print(data)\n",
        "print(F.softmax(data, dim=0))\n",
        "print(F.softmax(data, dim=0).sum())  # 確率分布であるから総和は 1 となる\n",
        "print(F.log_softmax(data, dim=0))  # ソフトマックス関数の対数変換"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([ 1.3800, -1.3505,  0.3455,  0.5046,  1.8213])\n",
            "tensor([0.2948, 0.0192, 0.1048, 0.1228, 0.4584])\n",
            "tensor(1.)\n",
            "tensor([-1.2214, -3.9519, -2.2560, -2.0969, -0.7801])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bo0u419tSMMT",
        "colab_type": "text"
      },
      "source": [
        "### 目的関数\n",
        "\n",
        "目的関数は学習に用いられ，ネットワークがその値を最小化されるよう訓練される関数です\n",
        "(*損失関数* または *コスト関数* とも呼ばれます)。\n",
        "これは，最初に 訓練インスタンス を選択し，ニューラルネットワークを実行して 出力関数の損失値を計算することで進行します。\n",
        "その後，モデルのパラメータは，損失関数の微分を取ることで更新されます。\n",
        "直感的には，モデルがその答えに完全に自信を持っていて，その答えが誤っている場合，損失関数の値は大きくなります。\n",
        "反対に，その答えに非常に自信を持っていて，かつ，その答えが正しければ、損失値は小さくなります。\n",
        "\n",
        "学習事例の損失関数を最小化するという考え方は，ネットワークの一般化に関与します。\n",
        "訓練データセット，とテストデータセット，を分割するのは，訓練環境での未見のデータでの損失が小さいことを期待するためです。\n",
        "損失関数の例としては、*負の対数尤度損失* があります。\n",
        "これは多クラス分類の非常に一般的な目的関数です。\n",
        "教師付き多クラス分類では，正しい出力の負の対数確率を最小にするようにネットワークを訓練することを意味します (正しい出力に対する対数確率を最大にする)。\n",
        "\n",
        "(訳注: 目的関数 ojbective function は，損失関数 loss function は，ほぼ同義で使われます。\n",
        "Goodfellow の教科書では objective, loss, goal, error function などは同義としてで扱う書かれています。)\n",
        "\n",
        "\n",
        "<!---\n",
        "The objective function is the function that your network is being trained to minimize (in which case it is often called a *loss function* or *cost function*). \n",
        "This proceeds by first choosing a training instance, running it through your neural network, and then computing the loss of the output. \n",
        "The parameters of the model are then updated by taking the derivative of the loss function. Intuitively, if your model is completely confident in its answer, and its answer is wrong, your loss will be high. \n",
        "If it is very confident in its answer, and its answer is correct, the loss will be low.\n",
        "\n",
        "The idea behind minimizing the loss function on your training examples is that your network will hopefully generalize well and have small loss on unseen examples in your dev set, test set, or in production. \n",
        "An example loss function is the *negative log likelihood loss*, which is a very common objective for multi-class classification. For supervised multi-class classification, this means training the network to minimize the negative log probability of the correct output (or equivalently, maximize the log probability of the correct output).\n",
        "-->\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5VrIW5-SMMU",
        "colab_type": "text"
      },
      "source": [
        "## 最適化と訓練\n",
        "\n",
        "<!--## Optimization and Training-->\n",
        "\n",
        "So what we can compute a loss function for an instance? \n",
        "What do we do with that? We saw earlier that Tensors know how to compute gradients with respect to the things that were used to compute it. \n",
        "Well, since our loss is an Tensor, we can compute gradients with respect to all of the parameters used to compute it! \n",
        "Then we can perform standard gradient updates. \n",
        "Let $\\theta$ be our parameters, $L(\\theta)$ the loss function, and $\\eta$ a positive learning rate. Then:\n",
        "\n",
        "ある事例 (データ) の損失関数を実際どう計算するのでしょうか？\n",
        "先ほど，PyTorch のテンソルは計算に使われたすべてについての勾配計算に関する情報を知っていると説明しました。\n",
        "損失関数は PyTorch テンソル なので，計算に使われたすべてのパラメータに関する勾配を計算することができます。\n",
        "標準的な勾配降下法によりパラメータ更新が可能です。\n",
        "$\\theta$ をパラメータとし，損失関数を $L(\\theta)$，正の学習率を $\\eta$ とすれば次式を得ます:\n",
        "\n",
        "<!--Let $\\theta$ is our parameters, $L(\\theta)$ the loss function, and $\\eta$ a positive learning rate. -->\n",
        "\n",
        "\\begin{align}\\theta^{(t+1)} = \\theta^{(t)} - \\eta \\nabla_\\theta L(\\theta)\\end{align}\n",
        "\n",
        "<!--There are a huge collection of algorithms and active research in attempting to do something more than just this vanilla gradient update.\n",
        "Many attempt to vary the learning rate based on what is happening at train time. \n",
        "You don't need to worry about what specifically these algorithms are doing unless you are really interested. \n",
        "Torch provides many in the torch.optim package, and they are all completely transparent. \n",
        "Using the simplest gradient update is the same as the more complicated algorithms. \n",
        "Trying different update algorithms and different parameters for the update algorithms (like different initial learning rates) is important in optimizing your network's performance. \n",
        "Often, just replacing vanilla SGD with an optimizer like Adam or RMSProp will boost performance noticably.-->\n",
        "\n",
        "このバニラ勾配降下法 による更新だけでなく，それ以上のことをしようとする試みには，膨大なアルゴリズムの研究が集積されていてり，活発な研究が行われています。\n",
        "(訳注: 何の飾り付けのない素のアルゴリズムという意味でしばしば `バニラ` vanilla という形容詞を用います。\n",
        "何のフレーバーもつかない素のアイスクリームをバニラアイスクリームと呼ぶように，\n",
        "例えば，教科書に記載されているような素の誤差逆伝播法のことを `バニラバックプロパゲーション` などと呼ぶ習慣があります)\n",
        "バニラではない，多くの発展的なアルゴリズムでは，訓練時に起こっていることに基づいて学習率を変化させます。\n",
        "本当に興味がない限り，これらのアルゴリズムが具体的に何をしているのかを気にする必要はありません。\n",
        "Torch では `torch.optim` パッケージで多くのアルゴリズムを提供しています。\n",
        "それらアルゴリズムはすべて完全に透過的です。すなわち，\n",
        "最も単純な勾配更新法を使うこととは，より複雑なアルゴリズムと同じく扱うことができます。\n",
        "ネットワークのパフォーマンスを最適化するには，さまざまな更新アルゴリズムや更新アルゴリズムのパラメータ (初期学習率の違いなど) を試すことが重要です。\n",
        "多くの場合， `バニラ SGD` を `Adam` や `RMSProp` のような最適化手法に置き換えるだけで，パフォーマンスが顕著に向上します。\n",
        "\n",
        "（訳注: バニラ SGD ではハイパーパラメータ $\\eta$ が固定です。\n",
        "一方 `RMSprop` 以降のアルゴリズムでは，各パラメータごとに異なる $\\eta$ を採用します。\n",
        "また，2 次微分 **ヘッセ行列** を用いるニュートン法の近似として，ヘッセ行列を近似式で置き換えた\n",
        "準ニュートン法のニューラルネットワーク的実装が `Adam` です。ニュートン法からの類推で明らかなように\n",
        "Adam では学習係数 $\\eta$ をヘシアンに応じて調整します。\n",
        "）\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSissIlkSMMU",
        "colab_type": "text"
      },
      "source": [
        "# PyTorch によるネットワークコンポネントの作成\n",
        "\n",
        "<!--Before we move on to our focus on NLP, lets do an annotated example of building a network in PyTorch using only affine maps and non-linearities. \n",
        "We will also see how to compute a loss function, using PyTorch's built in negative log likelihood, and update parameters by backpropagation.\n",
        "\n",
        "All network components should inherit from nn.Module and override the forward() method. \n",
        "That is about it, as far as the boilerplate is concerned. \n",
        "Inheriting from nn.Module provides functionality to your component. \n",
        "For example, it makes it keep track of its trainable parameters, you can swap it between CPU and GPU with the ``.to(device)`` method, where device can be a CPU device ``torch.device(\"cpu\")`` or CUDA device ``torch.device(\"cuda:0\")``. \n",
        "\n",
        "Let's write an annotated example of a network that takes in a sparse bag-of-words representation and outputs a probability distribution over two labels: \"English\" and \"Spanish\". This model is just logistic regression.\n",
        "-->\n",
        "\n",
        "NLP の話に移る前に，アフィン写像と非線形性だけを使って PyTorch でネットワークを構築してみましょう。\n",
        "加えて，PyTorch の負の対数尤度を使って損失関数を計算し，誤差逆伝播法 (バックプロパゲーション) でパラメータを更新する方法も見てみましょう。\n",
        "\n",
        "すべてのネットワークコンポーネントは `nn.Module` を継承し，`forward()` メソッドをオーバーライドする必要があります。\n",
        "ボイラープレート (訳注: いわゆるテンプレート版あるいはプロトタイプ，オブジェクト指向でのクラス定義的な意味合いです) に関する限りでは、これくらいです。\n",
        "`nn.Module` を継承することで コンポーネントに関数 (機能) を提供します。\n",
        "例えば 訓練可能なパラメータを管理したり  ``.to(device)`` メソッドを使って CPU と GPU を入れ替えたりすることができます。\n",
        "\n",
        "疎な bag-of-words 表現を受け取り，2 つのラベルの確率分布を出力するネットワークの注釈付きの例を書いてみましょう。\n",
        "「英語」と「スペイン語」の 2 つのラベルの確率を出力するネットワークの例を書いてみましょう。\n",
        "このモデルは単なるロジスティック回帰です。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFXgsXjgSMMU",
        "colab_type": "text"
      },
      "source": [
        "### 例: Bog-of-Words によるロジスティック回帰分類器\n",
        "\n",
        "<!--\n",
        "### Example: Logistic Regression Bag-of-Words classifier\n",
        "-->\n",
        "\n",
        "<!--Our model will map a sparse BoW representation to log probabilities over labels. \n",
        "We assign each word in the vocab an index. For example, say our entire vocab is two words \"hello\" and \"world\", with indices 0 and 1 respectively. The BoW vector for the sentence \"hello hello hello hello\" is-->\n",
        "\n",
        "モデルは 疎な BoW 表現をラベルの対数確率にマッピングするものです。\n",
        "語彙内の各単語にインデックスを割り当てます。例えば，全語彙が \"hello\" と \"world\" の 2 つの単語で成り立っているとします。\n",
        "それぞれ 0 と 1 のインデックスを持つとします。\n",
        "文 \"hello hello hello hello hello hello \" の BoW ベクトルは次のようになります。\n",
        "\n",
        "\\begin{align}\\left[ 4, 0 \\right]\\end{align}\n",
        "\n",
        "同様に，\"hello, workd world hello\" の BoW ベクトルは次のようになります:\n",
        "<!--For \"hello world world hello\", it is-->\n",
        "\n",
        "\\begin{align}\\left[ 2, 2 \\right]\\end{align}\n",
        "\n",
        "一般には，次のような表記となります:\n",
        "<!--etc. In general, it is-->\n",
        "\n",
        "\\begin{align}\\left[ \\text{Count}(\\text{hello}), \\text{Count}(\\text{world}) \\right]\\end{align}\n",
        "\n",
        "この BOW ベクトルを $x$ と表記します。ネットワークの出力は以下のとおり:\n",
        "\n",
        "<!--Denote this BOW vector as $x$. The output of our network is:-->\n",
        "\n",
        "\\begin{align}\\log \\text{Softmax}(Ax + b)\\end{align}\n",
        "\n",
        "すなわち入力信号を アフィン写像に通して，その値に対して log softmax を行います。\n",
        "<!--That is, we pass the input through an affine map and then do log softmax.-->\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qE4XOK7SMMV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "outputId": "fa31a84c-bbdc-4d50-f9a5-1b0575807a67"
      },
      "source": [
        "data = [(\"me gusta comer en la cafeteria\".split(), \"SPANISH\"),\n",
        "        (\"Give it to me\".split(), \"ENGLISH\"),\n",
        "        (\"No creo que sea una buena idea\".split(), \"SPANISH\"),\n",
        "        (\"No it is not a good idea to get lost at sea\".split(), \"ENGLISH\")]\n",
        "\n",
        "test_data = [(\"Yo creo que si\".split(), \"SPANISH\"),\n",
        "             (\"it is lost on me\".split(), \"ENGLISH\")]\n",
        "\n",
        "# word_to_ix は語彙中の各単語をとユニークな整数に変換します。\n",
        "word_to_ix = {}\n",
        "for sent, _ in data + test_data:\n",
        "    for word in sent:\n",
        "        if word not in word_to_ix:\n",
        "            word_to_ix[word] = len(word_to_ix)\n",
        "print(word_to_ix)\n",
        "\n",
        "VOCAB_SIZE = len(word_to_ix)\n",
        "NUM_LABELS = 2\n",
        "\n",
        "\n",
        "class BoWClassifier(nn.Module):  # inheriting from nn.Module!\n",
        "\n",
        "    def __init__(self, num_labels, vocab_size):\n",
        "        # nn.Module の init 関数を呼び出します。文法で混乱しないように。\n",
        "        # nn.Module での初期化が行われます\n",
        "        super(BoWClassifier, self).__init__()\n",
        "\n",
        "        # 必要なパラメータの定義。アフィン写像のパラメータ A と b \n",
        "        # Torch には nn.Linear() が定義されており，これがアフィン写像を提供します\n",
        "        # 入力次元が vocab_size であり，出力が num_labels である理由を理解してください。\n",
        "        self.linear = nn.Linear(vocab_size, num_labels)\n",
        "\n",
        "        # 非線形関数 log softmax にはパラメータがないことに注意\n",
        "\n",
        "    def forward(self, bow_vec):\n",
        "        # 入力を線形層に通し，さらに log_softmax に通します。\n",
        "        # 多くの非線形関数ややその他の関数は torch.nn.functional にあります\n",
        "        return F.log_softmax(self.linear(bow_vec), dim=1)\n",
        "\n",
        "\n",
        "def make_bow_vector(sentence, word_to_ix):\n",
        "    vec = torch.zeros(len(word_to_ix))\n",
        "    for word in sentence:\n",
        "        vec[word_to_ix[word]] += 1\n",
        "    return vec.view(1, -1)\n",
        "\n",
        "\n",
        "def make_target(label, label_to_ix):\n",
        "    return torch.LongTensor([label_to_ix[label]])\n",
        "\n",
        "\n",
        "model = BoWClassifier(NUM_LABELS, VOCAB_SIZE)\n",
        "\n",
        "# モデルはパラメータを知っています。 以下の最初の出力は A，2番目の出力は bです。\n",
        "# モジュールの __init__ 関数でクラス変数にコンポーネントを代入するときはいつも\n",
        "# self.linear = nn.Linear(...) 行で行われていました。\n",
        "# PyTorch 開発者による Python 魔法により，このモジュール (BoWClassifier) は\n",
        "# nn.Linear のパラメータの知識を保持しています\n",
        "for param in model.parameters():\n",
        "    print(param)\n",
        "\n",
        "# このモデルを走らせるためには，BoW ベクトルを通します\n",
        "# ここでは訓練する必要がないので，以下では torch.no_grad() で warp します\n",
        "with torch.no_grad():\n",
        "    sample = data[0]\n",
        "    bow_vector = make_bow_vector(sample[0], word_to_ix)\n",
        "    log_probs = model(bow_vector)\n",
        "    print(log_probs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'me': 0, 'gusta': 1, 'comer': 2, 'en': 3, 'la': 4, 'cafeteria': 5, 'Give': 6, 'it': 7, 'to': 8, 'No': 9, 'creo': 10, 'que': 11, 'sea': 12, 'una': 13, 'buena': 14, 'idea': 15, 'is': 16, 'not': 17, 'a': 18, 'good': 19, 'get': 20, 'lost': 21, 'at': 22, 'Yo': 23, 'si': 24, 'on': 25}\n",
            "Parameter containing:\n",
            "tensor([[ 0.1194,  0.0609, -0.1268,  0.1274,  0.1191,  0.1739, -0.1099, -0.0323,\n",
            "         -0.0038,  0.0286, -0.1488, -0.1392,  0.1067, -0.0460,  0.0958,  0.0112,\n",
            "          0.0644,  0.0431,  0.0713,  0.0972, -0.1816,  0.0987, -0.1379, -0.1480,\n",
            "          0.0119, -0.0334],\n",
            "        [ 0.1152, -0.1136, -0.1743,  0.1427, -0.0291,  0.1103,  0.0630, -0.1471,\n",
            "          0.0394,  0.0471, -0.1313, -0.0931,  0.0669,  0.0351, -0.0834, -0.0594,\n",
            "          0.1796, -0.0363,  0.1106,  0.0849, -0.1268, -0.1668,  0.1882,  0.0102,\n",
            "          0.1344,  0.0406]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.0631, 0.1465], requires_grad=True)\n",
            "tensor([[-0.5378, -0.8771]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oR3mB7vDSMMX",
        "colab_type": "text"
      },
      "source": [
        "<!--Which of the above values corresponds to the log probability of ENGLISH, and which to SPANISH? We never defined it, but we need to if we want to train the thing.-->\n",
        "\n",
        "上の値のうち、どれが ENGLISH の対数確率に対応し，どれが SPANISH の対数確率に対応するのでしょうか？\n",
        "これまで定義したことはありませんでしたが，もし学習したいのであれば定義する必要があります。\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foll1DltSMMY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label_to_ix = {\"SPANISH\": 0, \"ENGLISH\": 1}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-y9NcDb2SMMa",
        "colab_type": "text"
      },
      "source": [
        "<!--\n",
        "So lets train! \n",
        "To do this, we pass instances through to get log probabilities, compute a loss function, compute the gradient of the loss function, and then update the parameters with a gradient step. \n",
        "Loss functions are provided by Torch in the nn package. \n",
        "nn.NLLLoss() is the negative log likelihood loss we want. \n",
        "It also defines optimization functions in torch.optim. Here, we will just use SGD.\n",
        "\n",
        "Note that the *input* to NLL Loss is a vector of log probabilities, and a target label. \n",
        "It doesn't compute the log probabilities for us. \n",
        "This is why the last layer of our network is log softmax. The loss function nn.CrossEntropyLoss() is the same as NLLLoss(), except it does the log softmax for you.\n",
        "-->\n",
        "\n",
        "以下は訓練です。\n",
        "\n",
        "訓練には，インスタンスを通過させて対数確率を取得し，損失関数を計算し，損失関数の勾配を計算し，勾配ステップでパラメータを更新します。\n",
        "損失関数は Torch の nn パッケージ内で提供されています。\n",
        "`nn.NLLLoss()` は求める負の対数尤度損失です。 \n",
        " (訳注: NLL とは Negative Log Loss 関数，負の対数尤度を意味します)\n",
        "また 最適化関数も torch.optim で定義されています。ここでは SGD を使用します。\n",
        "\n",
        "NLL 損失への *input* は対数確率のベクトルとターゲットラベルであることに注意してください。\n",
        "この関数は 対数確率を計算してくれません。\n",
        "これが，このネットワークの最後の層が log softmax である理由です。\n",
        "損失関数 nn.CrossEntropyLoss() は NLLLoss() と同じですが、log softmax を計算してくれます。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JD9XFtzqSMMa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "2aaaff09-7e69-4877-ae12-9f8417ba5c7c"
      },
      "source": [
        "# 訓練前にテストデータを評価します。訓練前後での変化を見るためです\n",
        "with torch.no_grad():\n",
        "    for instance, label in test_data:\n",
        "        bow_vec = make_bow_vector(instance, word_to_ix)\n",
        "        log_probs = model(bow_vec)\n",
        "        print(log_probs)\n",
        "\n",
        "# \"creo\" に対応する行列の列を印字\n",
        "print(next(model.parameters())[:, word_to_ix[\"creo\"]])\n",
        "\n",
        "loss_function = nn.NLLLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "# 学習データを何度か通したいと思うでしょう.\n",
        "# 100 は実際のデータセットよりもはるかに大きいです。\n",
        "# ですが 実際のデータセットは 2 つ以上のインスタンスを持っています。\n",
        "# 通常 5 から 30 エポックの間のどこかが妥当です。\n",
        "for epoch in range(100):\n",
        "    for instance, label in data:\n",
        "        # ステップ 1. PyTorch には勾配が蓄積されていることを覚えておいてください\n",
        "        # 各インスタンスの前にその勾配を ゼロ消去する必要があります\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Step 2. Make our BOW vector and also we must wrap the target in a\n",
        "        # Tensor as an integer. For example, if the target is SPANISH, then\n",
        "        # we wrap the integer 0. The loss function then knows that the 0th\n",
        "        # element of the log probabilities is the log probability\n",
        "        # corresponding to SPANISH\n",
        "        \n",
        "        bow_vec = make_bow_vector(instance, word_to_ix)\n",
        "        target = make_target(label, label_to_ix)\n",
        "\n",
        "        # Step 3. Run our forward pass.\n",
        "        log_probs = model(bow_vec)\n",
        "\n",
        "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
        "        # calling optimizer.step()\n",
        "        loss = loss_function(log_probs, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for instance, label in test_data:\n",
        "        bow_vec = make_bow_vector(instance, word_to_ix)\n",
        "        log_probs = model(bow_vec)\n",
        "        print(log_probs)\n",
        "\n",
        "# Index corresponding to Spanish goes up, English goes down!\n",
        "print(next(model.parameters())[:, word_to_ix[\"creo\"]])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.9297, -0.5020]])\n",
            "tensor([[-0.6388, -0.7506]])\n",
            "tensor([-0.1488, -0.1313], grad_fn=<SelectBackward>)\n",
            "tensor([[-0.2093, -1.6669]])\n",
            "tensor([[-2.5330, -0.0828]])\n",
            "tensor([ 0.2803, -0.5605], grad_fn=<SelectBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geDvXNqzSMMc",
        "colab_type": "text"
      },
      "source": [
        "<!--We got the right answer! \n",
        "You can see that the log probability for Spanish is much higher in the first example, and the log probability for\n",
        "English is much higher in the second for the test data, as it should be.\n",
        "\n",
        "Now you see how to make a PyTorch component, pass some data through it and do gradient updates. \n",
        "We are ready to dig deeper into what deep NLP has to offer.\n",
        "-->\n",
        "\n",
        "正解が出ました! \n",
        "最初の例では，スペイン語の対数確率がはるかに高いことがわかります。\n",
        "英語では，テストデータの 2 番目の方が当然のように英語の方が高いです。\n",
        "\n",
        "これで PyTorch コンポーネントを作って，そこにいくつかのデータを渡して，勾配更新を行う方法がわかりました。\n",
        "これで ディープ NLP が提供するものをさらに深く掘り下げる準備が整いました。\n",
        "\n",
        "\n"
      ]
    }
  ]
}