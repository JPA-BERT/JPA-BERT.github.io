{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "2020-0723pytorch_tutorial.ipynb ",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JPA-BERT/jpa-bert.github.io/blob/master/notebooks/2020_0723pytorch_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2Ezm_N3Fag7",
        "colab_type": "text"
      },
      "source": [
        "# PyTorch tutorial の内容について\n",
        "\n",
        "[https://github.com/pytorch/tutorials/tree/master/beginner_source/nlp](https://github.com/pytorch/tutorials/tree/master/beginner_source/nlp) を見ると\n",
        "PyTorch で 自然言語処理を行う場合のチュートリアルは以下とおりである\n",
        "\n",
        "# Deep Learning for NLP with Pytorch\n",
        "\n",
        "1. [pytorch_tutorial.py](https://github.com/pytorch/tutorials/blob/master/beginner_source/nlp/pytorch_tutorial.py): \n",
        "\t[PyTorch 入門 Introduction to PyTorch](https://pytorch.org/tutorials/beginner/nlp/pytorch_tutorial.html)\n",
        "\n",
        "2. [deep_learning_tutorial.py](https://github.com/pytorch/tutorials/blob/master/beginner_source/nlp/deep_learning_tutorial.py): \n",
        "\t[PyTorch による深層学習 Deep Learning with PyTorch](https://pytorch.org/tutorials/beginner/nlp/deep_learning_tutorial.html)\n",
        "\n",
        "3. [word_embeddings_tutorial.py](https://github.com/pytorch/tutorials/blob/master/beginner_source/nlp/word_embeddings_tutorial.py): \n",
        "\t[単語埋め込み:語彙的意味の符号化 Word Embeddings: Encoding Lexical Semantics](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html)\n",
        "\n",
        "4. [sequence_models_tutorial.py]((https://github.com/pytorch/tutorials/blob/master/beginner_source/nlp/sequence_models_tutorial.py): \n",
        "\t[系列モデルと LSTM Sequence Models and Long-Short Term Memory Networks](https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html)\n",
        "\n",
        "5. [advanced_tutorial.py]((https://github.com/pytorch/tutorials/blob/master/beginner_source/nlp/advanced_tutorial.py): \n",
        "\t[動的意思決定と双方向 LSTM 条件付き確率場 Advanced: Making Dynamic Decisions and the Bi-LSTM CRF](https://pytorch.org/tutorials/beginner/nlp/advanced_tutorial.html)\n",
        "\n",
        "\n",
        "以下では，このうちの 1 について解説している。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPOlAhSfCeze",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uArW8eC0Cezi",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "- original: https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/7e00f82429a7b691e42bb5828dbb036e/pytorch_tutorial.ipynb\n",
        "- 注: この colab ファイルは PyTorch の公式チュートリアルからの翻訳に手を加えたものです。\n",
        "- date: 2020-0723\n",
        "---\n",
        "\n",
        "# Torch のテンソルライブラリ紹介 <!---Introduction to Torch's tensor library--->\n",
        "\n",
        "テンソルとは 2 次以上の次元を持つ行列の一般化であり，ディープラーニングモデルはすべてテンソルに関する計算です。\n",
        "これが何を意味するのかについては，後ほど詳しく説明します．まず，テンソルで何ができるかを見てみましょう．\n",
        "<!---\n",
        "All of deep learning is computations on tensors, which are generalizations of a matrix that can be indexed in more than 2 dimensions. \n",
        "We will see exactly what this means in-depth later. First, lets look what we can do with tensors.\n",
        "-->\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d13w0ClQCezi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "81c17e76-3c2b-4943-837f-d583cd117191"
      },
      "source": [
        "# Author: Robert Guthrie\n",
        "\n",
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.manual_seed(1)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f5a07d7c2d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QV0d8J6RCezk",
        "colab_type": "text"
      },
      "source": [
        "### テンソルの作成 \n",
        "\n",
        "テンソルは `torch.tesor()` 関数を使って作成される Python のリストです。\n",
        "<!--Tensors can be created from Python lists with the torch.tensor() function.-->\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "259Ae29ZCezl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "d20a6d76-ab66-414c-b7c4-81218fb08f71"
      },
      "source": [
        "# torch.tensor(data) により任意のデータを持つ  torch.Tensor オブジェクトが生成されます。 \n",
        "V_data = [1., 2., 3.]\n",
        "V = torch.tensor(V_data)\n",
        "print(V)\n",
        "\n",
        "# 行列を生成します \n",
        "M_data = [[1., 2., 3.], [4., 5., 6]]\n",
        "M = torch.tensor(M_data)\n",
        "print(M)\n",
        "\n",
        "# 2ｘ2ｘ2 の 3階テンソルを作成します\n",
        "T_data = [[[1., 2.], [3., 4.]],\n",
        "          [[5., 6.], [7., 8.]]]\n",
        "T = torch.tensor(T_data)\n",
        "print(T)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1., 2., 3.])\n",
            "tensor([[1., 2., 3.],\n",
            "        [4., 5., 6.]])\n",
            "tensor([[[1., 2.],\n",
            "         [3., 4.]],\n",
            "\n",
            "        [[5., 6.],\n",
            "         [7., 8.]]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJ5IcSjGCezn",
        "colab_type": "text"
      },
      "source": [
        "ここで  3 階テンソルとは何かについて考えてみます。\n",
        "ベクトルであれば，ベクトルについた添字 (たとえば $\\mathbf{x}_i$ における $i$) は，個々の要素すなわちスカラとなります。\n",
        "行列であれば，行列につけられた添字 ($\\mathbf{X}_i$) はベクトルとなります。（訳注，2 つの添字 $\\mathbf{X}_{ij}$ でスカラとなる）\n",
        "3 次元テンソル (3 階テンソル) であれば、テンソルにつけられた添字は行列になります。\n",
        "\n",
        "用語の注意：\n",
        "このチュートリアルで「テンソル」と言うときは、任意の `torch.Tensor` オブジェクトを指します。\n",
        "行列とベクトルは `torch.Tensors` の特殊なケースであり，次元はそれぞれ 1 と 2 です。\n",
        "3 次元テンソルについて話すときは、明示的に \"3次元テンソル \"という用語を使います。\n",
        "\n",
        "\n",
        "\n",
        "<!--What is a 3D tensor anyway? Think about it like this. \n",
        "If you have a vector, indexing into the vector gives you a scalar. \n",
        "If you have a matrix, indexing into the matrix gives you a vector. \n",
        "If you have a 3D  tensor, then indexing into the tensor gives you a matrix!\n",
        "\n",
        "A note on terminology:\n",
        "when I say \"tensor\" in this tutorial, it refers to any `torch.Tensor` object. \n",
        "Matrices and vectors are special cases of `torch.Tensors`, where their dimension is 1 and 2 respectively. \n",
        "When I am talking about 3D tensors, I will explicitly use the term \"3D tensor\".\n",
        "-->\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Zmyi8SNCezo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "b8998edc-f6e3-4100-886c-5b59bd3c3204"
      },
      "source": [
        "# Index into V and get a scalar (0 dimensional tensor)\n",
        "print(V[0])\n",
        "# Get a Python number from it\n",
        "print(V[0].item())\n",
        "\n",
        "# Index into M and get a vector\n",
        "print(M[0])\n",
        "\n",
        "# Index into T and get a matrix\n",
        "print(T[0])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(1.)\n",
            "1.0\n",
            "tensor([1., 2., 3.])\n",
            "tensor([[1., 2.],\n",
            "        [3., 4.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muh8XQZFCezq",
        "colab_type": "text"
      },
      "source": [
        "他のデータ型のテンソルを作成することもできます。\n",
        "整数型のテンソルを作るには、``torch.tensor([[1, 2], [3, 4]])`` を試してみてください。\n",
        "また、``dtype=torch.data_type`` を渡すことでデータ型を指定することもできます。\n",
        "その他のデータ型についてはドキュメントをチェックしてください。\n",
        "\n",
        "\n",
        "<!--You can also create tensors of other data types. \n",
        "To create a tensor of integer types, try torch.tensor([[1, 2], [3, 4]]) (where all elements in the list are integers).\n",
        "You can also specify a data type by passing in ``dtype=torch.data_type``.\n",
        "Check the documentation for more data types, but Float and Long will be the most common.\n",
        "-->\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSUVc4U3Cezq",
        "colab_type": "text"
      },
      "source": [
        "乱数でできたテンソルを作成するには `torch.randn()` を使ってください。\n",
        "(訳注: Numpy で正規乱数を発生させるには `np.random.randn()` である。この場合，与える引数は 1 つであり，生成する乱数の数である。\n",
        "テンソルに変換する際には `.reshape()` を用いる。`np.random.randn(3 * 4 * 5).reshape(3,4,5)` )\n",
        "<!--\n",
        "You can create a tensor with random data and the supplied dimensionality with `torch.randn()`\n",
        "-->\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFjzBY3pCezr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "50d459fa-b658-4912-bb74-eafd0e3fff1e"
      },
      "source": [
        "x = torch.randn((3, 4, 5))\n",
        "print(x)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[ 0.4100,  0.4085,  0.2579,  1.0950, -0.5065],\n",
            "         [ 0.0998, -0.6540,  0.7317, -1.4567,  1.6089],\n",
            "         [ 0.0938, -1.2597,  0.2546, -0.5020, -1.0412],\n",
            "         [ 0.7323,  1.3075, -1.1628,  0.1196, -0.1631]],\n",
            "\n",
            "        [[ 0.6614,  1.1899,  0.8165, -0.9135, -0.3538],\n",
            "         [ 0.7639, -0.5890, -0.7636,  1.3352,  0.6043],\n",
            "         [-0.1034, -0.1512,  1.2466,  0.5057,  0.9505],\n",
            "         [ 1.2966,  0.8738, -0.5603,  1.2858,  0.8168]],\n",
            "\n",
            "        [[-1.4648, -1.2629,  1.1220,  1.5663, -1.0371],\n",
            "         [-1.0669, -0.2085, -0.2155,  0.2705,  0.5597],\n",
            "         [-0.3184,  1.5117, -1.5208,  0.9196, -0.5484],\n",
            "         [-0.3472, -0.7474, -0.9234,  0.5734, -0.1093]]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mF1s0IhsCezt",
        "colab_type": "text"
      },
      "source": [
        "### テンソルの演算\n",
        "<!--Operations with Tensors-->\n",
        "\n",
        "<!--~~~~~~~~~~~~~~~~~~~~~~~\n",
        "\n",
        "You can operate on tensors in the ways you would expect.\n",
        "-->\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pm_DdY7DCezt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cfc6e3af-23a8-4673-ad2d-6a7bb9cfcde0"
      },
      "source": [
        "x = torch.tensor([1., 2., 3.])\n",
        "y = torch.tensor([4., 5., 6.])\n",
        "z = x + y\n",
        "print(z)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([5., 7., 9.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nen9DdXgCezw",
        "colab_type": "text"
      },
      "source": [
        "利用可能な膨大な数の操作の完全なリストは [`ドキュメント`](https://pytorch.org/docs/torch.html) を参照してください。\n",
        "これらの操作は数学的な操作だけではありません。\n",
        "\n",
        "後で使うことになる便利な操作の一つが 連結 `concatenate()` です。\n",
        "\n",
        "<!--\n",
        "See [`the documentation`](https://pytorch.org/docs/torch.html) for a complete list of the massive number of operations available to you. \n",
        "They expand beyond just mathematical operations.\n",
        "\n",
        "One helpful operation that we will make use of later is concatenation.\n",
        "-->\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ro4-XgnbCezw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "1dffd2ca-8a3d-4a94-fdc7-a926b23c21c8"
      },
      "source": [
        "# デフォルトでは，最初の次元にそって連結されます\n",
        "x_1 = torch.randn(2, 5)  # 2 行 5 列の行列 x_1 \n",
        "y_1 = torch.randn(3, 5)  # 3 行 5 列の行列 y_1\n",
        "z_1 = torch.cat([x_1, y_1])  # 連結すると 5 行 5 列の行列を得る\n",
        "print(z_1)\n",
        "\n",
        "# 列に沿った連結 Concatenate columns:\n",
        "x_2 = torch.randn(2, 3)  # 2 行 3 列の行列 x_2\n",
        "y_2 = torch.randn(2, 5)  # 2 行 5 列の行列 y_2\n",
        "# 第 2 引数で，連結する次元を指定する 0 は 行，1 なら 列\n",
        "z_2 = torch.cat([x_2, y_2], 1)\n",
        "print(z_2)\n",
        "\n",
        "# 2 つの行列が互換しない場合には，torch はエラーを吐く。下行をアンコメントしてエラーを確認してください\n",
        "# torch.cat([x_1, x_2])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 1.1296,  0.2214, -0.0558,  1.2057,  1.9486],\n",
            "        [-0.0766, -0.8562, -0.7870, -0.8161,  0.5470],\n",
            "        [-1.1707, -0.4699, -1.6271, -0.1127,  1.5980],\n",
            "        [-0.8445, -1.0489,  0.9387,  0.5378,  1.5372],\n",
            "        [-0.6943,  0.2174, -0.2995, -0.3749,  1.8673]])\n",
            "tensor([[ 0.9042,  0.1181,  1.8941,  1.1366, -1.9280, -0.5565, -0.1434,  2.6780],\n",
            "        [-0.4229,  0.7431,  0.0756,  0.6735,  0.4161, -1.0110,  0.5515,  0.9910]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiXBkI3CCezy",
        "colab_type": "text"
      },
      "source": [
        "## テンソルの形状変更 <!--Reshaping Tensors-->\n",
        "\n",
        "テンソルの形状を変更するには `.view()` メソッドを使用します。\n",
        "多くのニューラルネットワークコンポーネントは入力が特定の形状を持つことが仮定されています。\n",
        "このため `.vew()` メソッドは頻繁に使用されます。\n",
        "多くの場合 データをコンポーネントに渡す前に `.view()` で形状変更する必要があります。\n",
        "\n",
        "<!--\n",
        "Use the `.view()` method to reshape a tensor. \n",
        "This method receives heavy use, because many neural network components expect their inputs to have a certain shape. Often you will need to reshape before passing your data to the component.\n",
        "-->\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0W-OIRECezy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "f4ee6ca9-3066-4c62-dd1e-bf95ae76ae92"
      },
      "source": [
        "x = torch.randn(2, 3, 4)\n",
        "print(x)\n",
        "print(x.view(2, 12))  # 2 行 12 列の行列に形状変更 \n",
        "# 上と等価な操作が次行。ある次元が -1 であれば，形状を適宜類推してくれる\n",
        "print(x.view(2, -1))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[-0.3884, -0.0316, -0.5606, -0.6555],\n",
            "         [ 0.7262,  0.6789, -0.4302, -0.3849],\n",
            "         [ 0.7148,  0.3969,  0.1927,  0.3429]],\n",
            "\n",
            "        [[ 1.4161,  0.4738,  0.0827, -1.3034],\n",
            "         [ 0.7252, -0.5464, -0.8024,  0.0186],\n",
            "         [ 1.1349,  0.8658,  0.6334, -0.5392]]])\n",
            "tensor([[-0.3884, -0.0316, -0.5606, -0.6555,  0.7262,  0.6789, -0.4302, -0.3849,\n",
            "          0.7148,  0.3969,  0.1927,  0.3429],\n",
            "        [ 1.4161,  0.4738,  0.0827, -1.3034,  0.7252, -0.5464, -0.8024,  0.0186,\n",
            "          1.1349,  0.8658,  0.6334, -0.5392]])\n",
            "tensor([[-0.3884, -0.0316, -0.5606, -0.6555,  0.7262,  0.6789, -0.4302, -0.3849,\n",
            "          0.7148,  0.3969,  0.1927,  0.3429],\n",
            "        [ 1.4161,  0.4738,  0.0827, -1.3034,  0.7252, -0.5464, -0.8024,  0.0186,\n",
            "          1.1349,  0.8658,  0.6334, -0.5392]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8skymd9Cez1",
        "colab_type": "text"
      },
      "source": [
        "# 計算グラフ と自動微分 \n",
        "\n",
        "計算グラフの概念は、効率的な深層学習プログラミングに不可欠です。\n",
        "計算グラフは，データがどのように結合されて出力されるかを簡単に記述したものです．\n",
        "グラフはどのパラメータがどの操作に関与したかを完全に特定しているので、導関数を計算するのに十分な情報を含んでいます。\n",
        "曖昧に聞こえるかもしれませんが、基本フラグ ``requires_grad`` を使って何が起こっているのか見てみましょう。\n",
        "\n",
        "まず、プログラマの立場から考えてみましょう。\n",
        "先ほど作成した `torch.Tensor` オブジェクトには何が格納されているでしょうか？\n",
        "明らかにデータと形状，そして他にもいくつかあるかもしれません。\n",
        "しかし、2 つのテンソルを足し合わせると 出力テンソルが得られます。\n",
        "この出力テンソルが知っているのはデータと形状だけです。\n",
        "それが他の2つのテンソルの和であることは知りません\n",
        "（ファイルから読み込まれたのかもしれませんし、他の操作の結果かもしれません）。\n",
        "\n",
        "``requires_grad=True`` の場合，テンソルオブジェクトはどのようにして作られたかを追跡します。\n",
        "実際に見てみましょう。\n",
        "\n",
        "<!--\n",
        "The concept of a computation graph is essential to efficient deep learning programming, because it allows you to not have to write the back propagation gradients yourself. \n",
        "A computation graph is simply a specification of how your data is combined to give you the output. \n",
        "Since the graph totally specifies what parameters were involved with which operations, it contains enough information to compute derivatives. \n",
        "This probably sounds vague, so let's see what is going on using the fundamental flag ``requires_grad``.\n",
        "\n",
        "First, think from a programmers perspective. \n",
        "What is stored in the torch.Tensor objects we were creating above? \n",
        "Obviously the data and the shape, and maybe a few other things. But when we added two tensors together, we got an output tensor. \n",
        "All this output tensor knows is its data and shape. It has no idea that it was the sum of two other tensors (it could have been read in from a file, it could be the result of some other operation, etc.)\n",
        "\n",
        "If ``requires_grad=True``, the Tensor object keeps track of how it was created. Lets see it in action.\n",
        "-->\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvVCcVBNCez1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c21a1a2a-2723-4ec6-f79f-b315099f25a3"
      },
      "source": [
        "# PyTorch のテンソル操作には ``requries_grad`` フラグがあります\n",
        "x = torch.tensor([1., 2., 3], requires_grad=True)\n",
        "\n",
        "# requires_grad=True であれば，それ以前に行った演算操作を逆行できます \n",
        "y = torch.tensor([4., 5., 6], requires_grad=True)\n",
        "z = x + y\n",
        "print(z)\n",
        "\n",
        "# ですが z には追加情報必要 BUT z knows something extra.\n",
        "print(z.grad_fn)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([5., 7., 9.], grad_fn=<AddBackward0>)\n",
            "<AddBackward0 object at 0x7f5a077ce828>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnc9rHszCez3",
        "colab_type": "text"
      },
      "source": [
        "``z`` はそれがファイルから読み込まれたものではなく，乗算や指数などの結果でもないことを知っています。\n",
        "そして ``z.grad_fn`` を辿れば ``x`` と ``y`` にたどり着きます。\n",
        "\n",
        "では，どうやって勾配を計算するのに役立つのでしょうか？\n",
        "\n",
        "<!--\n",
        "So Tensors know what created them. z knows that it wasn't read in from a file, it wasn't the result of a multiplication or exponential or whatever. \n",
        "And if you keep following z.grad_fn, you will find yourself at x and y.\n",
        "\n",
        "But how does that help us compute a gradient?\n",
        "-->\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgjkPw_vCez3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "95a1d4f6-99d9-422a-b2f5-efffae952b75"
      },
      "source": [
        "# Lets sum up all the entries in z\n",
        "s = z.sum()\n",
        "print(s)\n",
        "print(s.grad_fn)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(21., grad_fn=<SumBackward0>)\n",
            "<SumBackward0 object at 0x7f5a077ce390>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pncS0SL2Cez6",
        "colab_type": "text"
      },
      "source": [
        "So now, what is the derivative of this sum with respect to the first component of x? In math, we want\n",
        "では，上記の和の微分で ``x`` の第一成分に対する微分は何でしょうか？ 数学的には:\n",
        "\n",
        "\\begin{align}\n",
        "\\frac{\\partial s}{\\partial x_0}\n",
        "\\end{align}\n",
        "\n",
        "``s`` はテンソル ``z`` の要素の和から作られたことを知っています。\n",
        "さらに ``z`` は，各要素が ``x + y`` から作られたことを知っています。従って:\n",
        "<!--Well, s knows that it was created as a sum of the tensor z. z knows that it was the sum x + y. So-->\n",
        "\n",
        "\\begin{align}\n",
        "s = \\overbrace{x_0 + y_0}^\\text{$z_0$} + \\overbrace{x_1 + y_1}^\\text{$z_1$} + \\overbrace{x_2 + y_2}^\\text{$z_2$}\n",
        "\\end{align}\n",
        "\n",
        "s は，微分を定めるための十分な情報を保持していおり，答え 1 を得ることができます。\n",
        "<!--And so s contains enough information to determine that the derivative we want is 1!-->\n",
        "\n",
        "自動微分により，微分を実際に計算する方法の問題が隠蔽されます。\n",
        "ここで重要ななことは ``s`` が微分を計算可能にする情報を保持しているということです。\n",
        "実際には Pytorch での開発では sum() と + 演算をプログラムして それらの勾配を計算する方法を知り、\n",
        "逆伝播アルゴリズムを実行します。\n",
        "このアルゴリズムの詳細な議論は このチュートリアルの範囲を超えています。\n",
        "\n",
        "<!--Of course this glosses over the challenge of how to actually compute that derivative. The point here is that s is carrying along enough information that it is possible to compute it. \n",
        "In reality, the developers of Pytorch program the sum() and + operations to know how to compute their gradients, and run the back propagation algorithm. \n",
        "An in-depth discussion of that algorithm is beyond the scope of this tutorial.\n",
        "-->\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ef_BXEtVCez6",
        "colab_type": "text"
      },
      "source": [
        "Pytorch に勾配を計算させて，正しく動作することを確認してみましょう。\n",
        "(このブロックを何度も実行すると 勾配が増加します。\n",
        "これは Pytorch が勾配を  *累積* し `.grad` プロパティに格納しているためです。\n",
        "多くのモデルではこの累積が非常に便利だからです)。\n",
        "\n",
        "\n",
        "<!--Lets have Pytorch compute the gradient, and see that we were right:\n",
        "(note if you run this block multiple times, the gradient will increment.\n",
        "That is because Pytorch *accumulates* the gradient into the .grad property, since for many models this is very convenient.)-->\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mnp1254Cez7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "203c9313-672f-4e72-c0f1-77cc68ef06cc"
      },
      "source": [
        "# calling .backward() on any variable will run backprop, starting from it.\n",
        "s.backward()\n",
        "print(x.grad)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1., 1., 1.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0U9twbUCez9",
        "colab_type": "text"
      },
      "source": [
        "下のセルで起こっていることを理解することがディープラーニングのプログラミングには非常に重要です。\n",
        "\n",
        "<!--Understanding what is going on in the block below is crucial for being a successful programmer in deep learning.-->\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBhhZmQTCez9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "881ecf82-1ce4-4f51-d8a7-5f9808f130f3"
      },
      "source": [
        "x = torch.randn(2, 2)\n",
        "y = torch.randn(2, 2)\n",
        "# デフォルトでは，ユーザが作成したテンソル逆向き勾配計算はしない設定 ``requrires_grad=False`` です\n",
        "print(x.requires_grad, y.requires_grad)\n",
        "z = x + y\n",
        "# z  を逆伝播させることができません。\n",
        "print(z.grad_fn)\n",
        "\n",
        "# ``.requires_grad_( ... )`` はテンソルが持つ ``requires_grad`` の値を変化させます。存在するchanges an existing Tensor's ``requires_grad``\n",
        "# インプレース，すなわち変数の値を直接書き換えます。(訳注:多く PyTorch メソッドではアンダースコアがあるとインプレースでの変換になる)\n",
        "# デフォルトでは ``Ture`` が設定される\n",
        "x = x.requires_grad_()\n",
        "y = y.requires_grad_()\n",
        "# 上で見たように z は勾配計算のための情報を保持している\n",
        "z = x + y\n",
        "print(z.grad_fn)\n",
        "# もしある操作に対する任意の入力で ``requrires_grad=True`` ではれば出力は以下のように成る\n",
        "print(z.requires_grad)\n",
        "\n",
        "# z は計算履歴をもち，x と y と関係づけられている。\n",
        "# この値を取り出すには **detatch** して履歴から切り離す\n",
        "new_z = z.detach()\n",
        "\n",
        "# 上の ``new_z`` は逆向計算のための情報を保持敷いていのだろうか？\n",
        "# 否! \n",
        "print(new_z.grad_fn)\n",
        "# ``z.detach()`` は記憶装置に保持されているテンソル z と同じ内容を返す。\n",
        "# だが，計算履歴は忘却する。どのようにして計算されたのかという情報を保持していない\n",
        "# すなわち，`.detach()` メソッドはそのテンソルの過去の履歴を破壊する"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False False\n",
            "None\n",
            "<AddBackward0 object at 0x7f5a077ced68>\n",
            "True\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PAtAEAfCez_",
        "colab_type": "text"
      },
      "source": [
        "``with torch.no_grad():`` ブロックで包むことで，`requrires_grad=True` としてあったテンソルの履歴追跡を止めることができる。\n",
        "\n",
        "<!--You can also stop autograd from tracking history on Tensors with ``.requires_grad``=True by wrapping the code block in ``with torch.no_grad():``\n",
        "-->\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFPqZj_MCez_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "dd497aab-e9c8-4310-82e5-47979056f056"
      },
      "source": [
        "print(x.requires_grad)\n",
        "print((x ** 2).requires_grad)\n",
        "\n",
        "with torch.no_grad():\n",
        "\tprint((x ** 2).requires_grad)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "True\n",
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGq4kDFMRYuE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
