{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03PyTorchTEXT_char_rnn_generation_tutorial.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JPA-BERT/jpa-bert.github.io/blob/master/notebooks/03PyTorchTEXT_char_rnn_generation_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSBn_wGt0t0a",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "    \n",
        "このファイルは PyTorch のチュートリアルにあるファイル <https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html> を翻訳して，加筆修正したもの\n",
        "です。\n",
        "\n",
        "すぐれたチュートリアルの内容，コードを公開された Sean Robertson と PyTorch 開発陣に敬意を表します。\n",
        "\n",
        "- Original: \n",
        "- Date: 2020-0811\n",
        "- Translated and modified: Shin Asakawa <asakawa@ieee.org>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7A-xHnZB0t0b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from https://github.com/dmlc/xgboost/issues/1715\n",
        "import os\n",
        "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pM3e8RZSirWX",
        "colab": {}
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6P2d_Sb6irWw"
      },
      "source": [
        "# ゼロからの NLP (自然言語処理): 文字レベル RNN による苗字の生成\n",
        "<!--\n",
        "# NLP From Scratch: Generating Names with a Character-Level RNN\n",
        "-->\n",
        "\n",
        "**Author**: [Sean Robertson](https://github.com/spro/practical-pytorch)\n",
        "\n",
        "「ゼロからの NLP」の 3 つのチュートリアルの 第2回目です。\n",
        "[1 回目のチュートリアル](intermediate/char_rnn_classification_tutorial) \n",
        "(訳注: この翻訳では <>)\n",
        "では RNN を使って名前をその言語に分類しました。\n",
        "今回は裏を返せば，言語から名前を生成します。\n",
        "\n",
        "<!--\n",
        "This is our second of three tutorials on \"NLP From Scratch\". \n",
        "In the [first tutorial](intermediate/char_rnn_classification_tutorial), we used a RNN to classify names into their language of origin. \n",
        "This time we'll turn around and generate names from languages.\n",
        "-->\n",
        "\n",
        "```bash\n",
        "$ python sample.py Russian RUS\n",
        "Rovakov\n",
        "Uantov\n",
        "Shavakov\n",
        "\n",
        "$ python sample.py German GER\n",
        "Gerren\n",
        "Ereng\n",
        "Rosher\n",
        "\n",
        "$ python sample.py Spanish SPA\n",
        "Salla\n",
        "Parer\n",
        "Allan\n",
        "\n",
        "$ python sample.py Chinese CHI\n",
        "Chan\n",
        "Hang\n",
        "Iun\n",
        "```\n",
        "\n",
        "We are still hand-crafting a small RNN with a few linear layers. \n",
        "The big difference is instead of predicting a category after reading in all the letters of a name, we input a category and output one letter at a time.\n",
        "Recurrently predicting characters to form language (this could also be done with words or other higher order constructs) is often referred to as a \"language model\".\n",
        "\n",
        "\n",
        "**推薦図書:**\n",
        "<!--**Recommended Reading:**-->\n",
        "\n",
        "<!--I assume you have at least installed PyTorch, know Python, and understand Tensors:-->\n",
        "\n",
        "このチュートリアルでは，最低限，インストール済 PyTorch，Python を知っていること，Tensors を理解していることを前提としています。\n",
        "\n",
        "- https://pytorch.org/ インストール方法について<!--For installation instructions-->\n",
        "- [deep_learning_60min_blitz](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html) PyTorch を始めるには <!--to get started with PyTorch in general-->\n",
        "- [pytorch_with_examples](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html) 広範囲で深い概観 <!--for a wide and deep overview-->\n",
        "- [former_torchies_tutorial](https://pytorch.org/tutorials/beginner/former_torchies_tutorial.html?highlight=former%20tutorials) Lua ユーザのためのチュートリアル <!--if you are former Lua Torch user-->\n",
        "\n",
        "リカレントニューラルネットワークとその動作について有益な情報:\n",
        "<!--It would also be useful to know about RNNs and how they work:-->\n",
        "\n",
        "-  [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) 有益な事例の例証 <!--shows a bunch of real life examples -->\n",
        "-  [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) 具体的には LSTM についてですが，一般的には RNN についても参考になります。<!--is about LSTMs specifically but also informative about RNNs in  general-->\n",
        "\n",
        "## データの準備\n",
        "<!--\n",
        "## Preparing the Data\n",
        "-->\n",
        "\n",
        "### 覚書:\n",
        "<!--\n",
        "### Note:\n",
        "-->\n",
        "\n",
        "ここからデータをダウンロードして，カレントディレクトリに解凍してください\n",
        "<!--\n",
        "Download the data from [here](https://download.pytorch.org/tutorial/data.zip) and extract it to the current directory.\n",
        "-->\n",
        "\n",
        "``data/names`` ディレクトリは 18 個のテキストファイルがあります。各々 \"\\[Language\\].txt\" というファイル名です。\n",
        "各ファイルは各行に一つ名前が書かれています。順番はランダマイズされていますが，ユニコードをアスキー文字に変換する必要があります。\n",
        "<!--\n",
        "Included in the ``data/names`` directory are 18 text files named as \"[Language].txt\". \n",
        "Each file contains a bunch of names, one name per line, mostly romanized (but we still need to convert from Unicode to ASCII).\n",
        "-->\n",
        "\n",
        "最終的には言語ごとの名前のリストの辞書 ``{language: [names ...]}`` です。\n",
        "一般的な変数 \"category\" と \"line\"（ここでは言語と名前を表す）は後で必要となる拡張性のためです。\n",
        "<!--\n",
        "We'll end up with a dictionary of lists of names per language, ``{language: [names ...]}``. \n",
        "The generic variables \"category\" and \"line\" (for language and name in our case) are used for later extensibility.\n",
        "-->\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArzSGvWG0t0k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://download.pytorch.org/tutorial/data.zip\n",
        "!unzip data.zip   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2OVuKuaeirWz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "af48dc0d-cacb-47e9-c644-74b719aaad0f"
      },
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import glob\n",
        "import os\n",
        "import unicodedata\n",
        "import string\n",
        "\n",
        "all_letters = string.ascii_letters + \" .,;'-\"\n",
        "n_letters = len(all_letters) + 1 # Plus EOS marker\n",
        "\n",
        "def findFiles(path): return glob.glob(path)\n",
        "\n",
        "# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "        and c in all_letters\n",
        "    )\n",
        "\n",
        "# Read a file and split into lines\n",
        "def readLines(filename):\n",
        "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
        "    return [unicodeToAscii(line) for line in lines]\n",
        "\n",
        "# Build the category_lines dictionary, a list of lines per category\n",
        "category_lines = {}\n",
        "all_categories = []\n",
        "for filename in findFiles('data/names/*.txt'):\n",
        "    category = os.path.splitext(os.path.basename(filename))[0]\n",
        "    all_categories.append(category)\n",
        "    lines = readLines(filename)\n",
        "    category_lines[category] = lines\n",
        "\n",
        "n_categories = len(all_categories)\n",
        "\n",
        "if n_categories == 0:\n",
        "    raise RuntimeError('Data not found. Make sure that you downloaded data '\n",
        "        'from https://download.pytorch.org/tutorial/data.zip and extract it to '\n",
        "        'the current directory.')\n",
        "\n",
        "print('# categories:', n_categories, all_categories)\n",
        "print(unicodeToAscii(\"O'Néàl\"))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# categories: 18 ['Portuguese', 'Greek', 'Arabic', 'Korean', 'Italian', 'Scottish', 'Vietnamese', 'Japanese', 'German', 'Russian', 'Polish', 'Czech', 'Irish', 'French', 'Spanish', 'English', 'Dutch', 'Chinese']\n",
            "O'Neal\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IfjocISAirW7"
      },
      "source": [
        "## ネットワークの作成\n",
        "\n",
        "<!--\n",
        "## Creating the Network\n",
        "-->\n",
        "\n",
        "<!--\n",
        "This network extends [the last tutorial's RNN](Creating-the-Network) with an extra argument for the category tensor, which is concatenated along with the others. \n",
        "The category tensor is a one-hot vector just like the letter input.\n",
        "-->\n",
        "\n",
        "このネットワークは、[前回のチュートリアルの RNN](Creating-the-Network)を拡張したものです。\n",
        "カテゴリテンソルのための追加引数を持ち，他の引数と一緒に連結されます。\n",
        "カテゴリテンソルは，文字入力と同じように 1 つのホットベクトルです。\n",
        "\n",
        "<!--\n",
        "We will interpret the output as the probability of the next letter. \n",
        "When sampling, the most likely output letter is used as the next input letter.\n",
        "-->\n",
        "\n",
        "出力は次の文字の確率として解釈します。\n",
        "サンプリングの際には，出力される可能性の高い文字を次の入力文字として使用します。\n",
        "\n",
        "I added a second linear layer ``o2o`` (after combining hidden and output) to give it more muscle to work with. \n",
        "There's also a dropout layer, which [randomly zeros parts of its input](https://arxiv.org/abs/1207.0580) with a given probability (here 0.1) and is usually used to fuzz inputs to prevent overfitting.\n",
        "Here we're using it towards the end of the network to purposely add some chaos and increase sampling variety.\n",
        "\n",
        "2つ目の線形層 ``o2o`` (hidden と output を組み合わせた後に) を追加して，作業にメリハリをつけるようにしました。\n",
        "ドロップアウト層もあります。これは [入力の一部をランダムにゼロにする](https://arxiv.org/abs/1207.0580) というものです。\n",
        "与えられた確率 (ここでは 0.1)で，通常はオーバーフィットを防ぐために入力をファズさせるのに使われます。\n",
        "ここでは意図的にカオスを加えてサンプリングの多様性を高めるために，ネットワークの最後に使用しています。\n",
        "\n",
        "<!--\n",
        ".. figure:: https://i.imgur.com/jzVrf7f.png\n",
        "   :alt:\n",
        "-->\n",
        "\n",
        "\n",
        "<img src=\"https://i.imgur.com/jzVrf7f.png\" style=\"width:33%\">\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zQ8SBX6birW7",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(RNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.i2h = nn.Linear(n_categories + input_size + hidden_size, hidden_size)\n",
        "        self.i2o = nn.Linear(n_categories + input_size + hidden_size, output_size)\n",
        "        self.o2o = nn.Linear(hidden_size + output_size, output_size)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, category, input, hidden):\n",
        "        input_combined = torch.cat((category, input, hidden), 1)\n",
        "        hidden = self.i2h(input_combined)\n",
        "        output = self.i2o(input_combined)\n",
        "        output_combined = torch.cat((hidden, output), 1)\n",
        "        output = self.o2o(output_combined)\n",
        "        output = self.dropout(output)\n",
        "        output = self.softmax(output)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, self.hidden_size)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iih8n_YnirW_"
      },
      "source": [
        "# 訓練\n",
        "<!--\n",
        "## Training\n",
        "-->\n",
        "\n",
        "## 訓練の準備\n",
        "<!--\n",
        "## Preparing for Training\n",
        "-->\n",
        "\n",
        "最初に，カテゴリと行のランダム対を得るためのヘルパー関数を定義します:\n",
        "<!--\n",
        "First of all, helper functions to get random pairs of (category, line):\n",
        "-->\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "axcYghQRirXB",
        "colab": {}
      },
      "source": [
        "import random\n",
        "\n",
        "# Random item from a list\n",
        "def randomChoice(l):\n",
        "    return l[random.randint(0, len(l) - 1)]\n",
        "\n",
        "# Get a random category and random line from that category\n",
        "def randomTrainingPair():\n",
        "    category = randomChoice(all_categories)\n",
        "    line = randomChoice(category_lines[category])\n",
        "    return category, line"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "r9fHf483irXF"
      },
      "source": [
        "<!--\n",
        "For each timestep (that is, for each letter in a training word) the inputs of the network will be ``(category, current letter, hidden state)`` and the outputs will be ``(next letter, next hidden state)``. So for each training set, we'll need the category, a set of input letters, and a set of output/target letters.\n",
        "-->\n",
        "\n",
        "各タイムステップ(つまり学習単語の中の各文字)において，ネットワークの入力は ``(カテゴリ, 現在の文字, 隠れた状態)`` となります。\n",
        "出力は ``(次の文字, 次の隠れた状態)`` です。\n",
        "したがって，それぞれの学習セットに対して，カテゴリ，入力文字のセット，出力/ターゲット文字のセットが必要になります。\n",
        "\n",
        "<!--\n",
        "Since we are predicting the next letter from the current letter for each timestep, the letter pairs are groups of consecutive letters from the line - e.g. for ``\"ABCD<EOS>\"`` we would create (\"A\", \"B\"), (\"B\", \"C\"), (\"C\", \"D\"), (\"D\", \"EOS\").\n",
        "-->\n",
        "\n",
        "各タイムステップで現在の文字から次の文字を予測しているので，文字ペアはその行の連続した文字のグループになります - 例えば，\"ABCD<EOS>\" の場合 (\"A\", \"B\"), (\"B\", \"C\"), (\"C\", \"D\"), (\"D\", \"EOS\")となります。\n",
        "\n",
        "\n",
        "<!--\n",
        ".. figure:: https://i.imgur.com/JH58tXY.png\n",
        "   :alt:\n",
        "-->\n",
        "\n",
        "<img src=\"https://i.imgur.com/JH58tXY.png\" style=\"width:66%\">\n",
        "\n",
        "<!--\n",
        "The category tensor is a [one-hot tensor](https://en.wikipedia.org/wiki/One-hot) of size ``<1 x n_categories>``. \n",
        "When training we feed it to the network at every timestep - this is a design choice, it could have been included as part of initial hidden state or some other strategy.\n",
        "-->\n",
        "    \n",
        "カテゴリテンソルはサイズ ``[1 x n_categories]``  の [ワンホットテンソル](https://en.wikipedia.org/wiki/One-hot) です。\n",
        "これは設計上の選択であり，初期の隠れ層や他の戦略の一部として含まれている可能性があります。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oPnqgNFYirXI",
        "colab": {}
      },
      "source": [
        "# One-hot vector for category\n",
        "def categoryTensor(category):\n",
        "    li = all_categories.index(category)\n",
        "    tensor = torch.zeros(1, n_categories)\n",
        "    tensor[0][li] = 1\n",
        "    return tensor\n",
        "\n",
        "# One-hot matrix of first to last letters (not including EOS) for input\n",
        "def inputTensor(line):\n",
        "    tensor = torch.zeros(len(line), 1, n_letters)\n",
        "    for li in range(len(line)):\n",
        "        letter = line[li]\n",
        "        tensor[li][0][all_letters.find(letter)] = 1\n",
        "    return tensor\n",
        "\n",
        "# LongTensor of second letter to end (EOS) for target\n",
        "def targetTensor(line):\n",
        "    letter_indexes = [all_letters.find(line[li]) for li in range(1, len(line))]\n",
        "    letter_indexes.append(n_letters - 1) # EOS\n",
        "    return torch.LongTensor(letter_indexes)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7PwCO1q9irXq"
      },
      "source": [
        "<!--\n",
        "For convenience during training we'll make a ``randomTrainingExample`` function that fetches a random (category, line) pair and turns them into the required (category, input, target) tensors.\n",
        "-->\n",
        "\n",
        "学習中に便利なように ``randomTrainingExample`` 関数を作ってみましょう。\n",
        "訓練中にランダムな (カテゴリと行との)対を取得し，それを必要な(カテゴリ, 入力, 目標) テンソルに変換する簡単な ``randomTrainingExample`` 関数を作ってみましょう。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9VaeUAKSirXt",
        "colab": {}
      },
      "source": [
        "# Make category, input, and target tensors from a random category, line pair\n",
        "def randomTrainingExample():\n",
        "    category, line = randomTrainingPair()\n",
        "    category_tensor = categoryTensor(category)\n",
        "    input_line_tensor = inputTensor(line)\n",
        "    target_line_tensor = targetTensor(line)\n",
        "    return category_tensor, input_line_tensor, target_line_tensor"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "U7fGhg_CirYI"
      },
      "source": [
        "## ネットワークを訓練する\n",
        "<!--\n",
        "## Training the Network\n",
        "-->\n",
        "\n",
        "<!--\n",
        "In contrast to classification, where only the last output is used, we are making a prediction at every step, so we are calculating loss at every step.\n",
        "\n",
        "The magic of autograd allows you to simply sum these losses at each step and call backward at the end.\n",
        "-->\n",
        "\n",
        "最後の出力だけを使う分類とは違い，ステップごとに予測をしているのでステップごとに損失を計算しています。\n",
        "\n",
        "自動微分のマジックを使えば，これらの損失を各ステップで単純に合計して，最後に逆算して呼び出すことができます。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "emWoCt3firYI",
        "colab": {}
      },
      "source": [
        "criterion = nn.NLLLoss()\n",
        "\n",
        "learning_rate = 0.0005\n",
        "\n",
        "def train(category_tensor, input_line_tensor, target_line_tensor):\n",
        "    target_line_tensor.unsqueeze_(-1)\n",
        "    hidden = rnn.initHidden()\n",
        "\n",
        "    rnn.zero_grad()\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for i in range(input_line_tensor.size(0)):\n",
        "        output, hidden = rnn(category_tensor, input_line_tensor[i], hidden)\n",
        "        l = criterion(output, target_line_tensor[i])\n",
        "        loss += l\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    for p in rnn.parameters():\n",
        "        p.data.add_(p.grad.data, alpha=-learning_rate)\n",
        "\n",
        "    return output, loss.item() / input_line_tensor.size(0)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BesZ2dNhirYR"
      },
      "source": [
        "<!--\n",
        "To keep track of how long training takes I am adding a ``timeSince(timestamp)`` function which returns a human readable string:\n",
        "-->\n",
        "\n",
        "訓練に要した時間を記録するため，人間が読める文字列を返す ``timeSince(timestamp)`` 関数を追加しています。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NRFTmii-irYS",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "def timeSince(since):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EVii19R9irYV"
      },
      "source": [
        "<!--\n",
        "Training is business as usual - call train a bunch of times and wait a few minutes, printing the current time and loss every ``print_every`` examples, and keeping store of an average loss per ``plot_every`` examples in ``all_losses`` for plotting later.\n",
        "-->\n",
        "\n",
        "訓練はいつものように行います。\n",
        "`train` を何度も呼び出して数分待ち，現在の時間と損失を `print_every` の例ごとに表示します。\n",
        "後でプロットするために `plot_every` の例ごとの平均損失を `all_losses` の中に保存します。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vqV3i6t5irYW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "7c9b1bb8-996e-45b8-b02f-7049137172ed"
      },
      "source": [
        "rnn = RNN(n_letters, 128, n_letters)\n",
        "\n",
        "n_iters = 100000\n",
        "print_every = 5000\n",
        "plot_every = 500\n",
        "all_losses = []\n",
        "total_loss = 0 # Reset every plot_every iters\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "for iter in range(1, n_iters + 1):\n",
        "    output, loss = train(*randomTrainingExample())\n",
        "    total_loss += loss\n",
        "\n",
        "    if iter % print_every == 0:\n",
        "        print('%s (%d %d%%) %.4f' % (timeSince(start), iter, iter / n_iters * 100, loss))\n",
        "\n",
        "    if iter % plot_every == 0:\n",
        "        all_losses.append(total_loss / plot_every)\n",
        "        total_loss = 0"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0m 14s (5000 5%) 2.8376\n",
            "0m 28s (10000 10%) 2.6974\n",
            "0m 42s (15000 15%) 3.0707\n",
            "0m 57s (20000 20%) 2.6628\n",
            "1m 11s (25000 25%) 2.9733\n",
            "1m 26s (30000 30%) 3.0265\n",
            "1m 40s (35000 35%) 2.4190\n",
            "1m 55s (40000 40%) 1.9681\n",
            "2m 9s (45000 45%) 2.0322\n",
            "2m 24s (50000 50%) 2.2317\n",
            "2m 38s (55000 55%) 1.9612\n",
            "2m 52s (60000 60%) 2.3128\n",
            "3m 7s (65000 65%) 2.5840\n",
            "3m 21s (70000 70%) 1.2222\n",
            "3m 35s (75000 75%) 2.0350\n",
            "3m 49s (80000 80%) 3.4048\n",
            "4m 4s (85000 85%) 2.0767\n",
            "4m 18s (90000 90%) 1.9382\n",
            "4m 32s (95000 95%) 3.5228\n",
            "4m 47s (100000 100%) 1.6025\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "i7RERgbFirYd"
      },
      "source": [
        "## 損失関数のプロット\n",
        "<!--\n",
        "## Plotting the Losses\n",
        "-->\n",
        "\n",
        "<!--Plotting the historical loss from all\\_losses shows the network learning:-->\n",
        "\n",
        "すべての損失から損失値の履歴をプロットすると ネットワークの学習過程を示します:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hQCNKfMVirYd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "eda296ad-84d4-476c-8b51-1cee28ac933e"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(all_losses)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f3e14a0e518>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VS0N9VRGirYh"
      },
      "source": [
        "## ネットワークのサンプリング\n",
        "<!--\n",
        "## Sampling the Network\n",
        "-->\n",
        "\n",
        "<!--\n",
        "To sample we give the network a letter and ask what the next one is, feed that in as the next letter, and repeat until the EOS token.\n",
        "-->\n",
        "\n",
        "<!--サンプルとして-->\n",
        "ネットワークに文字を与え，次の文字は何かを尋ね，それを次の文字として入力し，EOSトークンまで繰り返します。\n",
        "\n",
        "```\n",
        "- 入力カテゴリ，開始文字，ゼロで初期化した中間層テンソルを作成する\n",
        "- 文字列 ``output_name`` を作成する\n",
        "- 最大出力長まで以下を繰り返す\n",
        "    - 現在の文字をネットワークに送る\n",
        "    - 最高出力値から次の文字を出力し，次の中間層態を取得する\n",
        "    - 文字が EOS の場合はここで中止する\n",
        "    - 通常の文字の場合は ``output_name`` に追加して次のステップを続ける\n",
        "- 最終的な名前を返す\n",
        "```\n",
        "\n",
        "<!--\n",
        "- Create tensors for input category, starting letter, and empty hidden state\n",
        "- Create a string ``output_name`` with the starting letter\n",
        "- Up to a maximum output length,\n",
        "    - Feed the current letter to the network\n",
        "    - Get the next letter from highest output, and next hidden state\n",
        "    - If the letter is EOS, stop here\n",
        "    - If a regular letter, add to ``output_name`` and continue\n",
        "- Return the final name\n",
        "-->\n",
        "\n",
        "### 覚書:\n",
        "\n",
        "開始文字を与えるのではなく，別の戦略として，学習時に「文字列の開始」トークンを含め，ネットワークに独自の開始文字を選択させるという方法もあったでしょう。\n",
        "\n",
        "<!--\n",
        ".. Note::\n",
        "Rather than having to give it a starting letter, another strategy would have been to include a \"start of string\" token in training and have the network choose its own starting letter.\n",
        "-->\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Zk8NNPU5irYh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "d1e4769f-f222-47d3-f5d7-12711f805039"
      },
      "source": [
        "max_length = 20\n",
        "\n",
        "# Sample from a category and starting letter\n",
        "def sample(category, start_letter='A'):\n",
        "    with torch.no_grad():  # no need to track history in sampling\n",
        "        category_tensor = categoryTensor(category)\n",
        "        input = inputTensor(start_letter)\n",
        "        hidden = rnn.initHidden()\n",
        "\n",
        "        output_name = start_letter\n",
        "\n",
        "        for i in range(max_length):\n",
        "            output, hidden = rnn(category_tensor, input[0], hidden)\n",
        "            topv, topi = output.topk(1)\n",
        "            topi = topi[0][0]\n",
        "            if topi == n_letters - 1:\n",
        "                break\n",
        "            else:\n",
        "                letter = all_letters[topi]\n",
        "                output_name += letter\n",
        "            input = inputTensor(letter)\n",
        "\n",
        "        return output_name\n",
        "\n",
        "# Get multiple samples from one category and multiple starting letters\n",
        "def samples(category, start_letters='ABC'):\n",
        "    for start_letter in start_letters:\n",
        "        print(sample(category, start_letter))\n",
        "\n",
        "\n",
        "print('ロシア人')\n",
        "samples('Russian', 'RUS')\n",
        "\n",
        "print('\\nドイツ人')\n",
        "samples('German', 'GER')\n",
        "\n",
        "print('\\nスペイン人')\n",
        "samples('Spanish', 'SPA')\n",
        "\n",
        "print('\\n中国人')\n",
        "samples('Chinese', 'CHI')\n",
        "\n",
        "print('\\n日本人')\n",
        "samples('Japanese', 'JPA')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ロシア人\n",
            "Rovakov\n",
            "Uoverin\n",
            "Shilovev\n",
            "\n",
            "ドイツ人\n",
            "Gerren\n",
            "Erent\n",
            "Roull\n",
            "\n",
            "スペイン人\n",
            "Salla\n",
            "Parer\n",
            "Allara\n",
            "\n",
            "中国人\n",
            "Chan\n",
            "Han\n",
            "Iun\n",
            "\n",
            "日本人\n",
            "Jamas\n",
            "Pamara\n",
            "Araki\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nLEWy-nLirYk"
      },
      "source": [
        "## 演習\n",
        "<!--\n",
        "## Exercises\n",
        "\n",
        "- Try with a different dataset of category -> line, for example:\n",
        "    - Fictional series -> Character name\n",
        "    - Part of speech -> Word\n",
        "    - Country -> City\n",
        "\n",
        "- Use a \"start of sentence\" token so that sampling can be done without choosing a start letter\n",
        "- Get better results with a bigger and/or better shaped network\n",
        "    - Try the nn.LSTM and nn.GRU layers\n",
        "    - Combine multiple of these RNNs as a higher level network\n",
        "-->\n",
        "\n",
        "- カテゴリ -> セリフなど 別のデータセットで試してみよ\n",
        "    - 虚構シリーズ -> キャラクター名\n",
        "    - 品詞 -> 単語\n",
        "    - 国 -> 都市\n",
        "\n",
        "- 文頭文字を選択しなくてもサンプリングができるように、「文頭」トークンを使用する\n",
        "- より大きなネットワークとより良い形のネットワークで より良い結果を得ることができる\n",
        "    - nn.LSTM と nn.GRU レイヤーを試せ\n",
        "    - これらの複数の RNN を高次ネットワークとして組み合わよ\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONbvHLku0t1V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}