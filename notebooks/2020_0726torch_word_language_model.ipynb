{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "2020-0726torch_word_language_model.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JPA-BERT/jpa-bert.github.io/blob/master/notebooks/2020_0726torch_word_language_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhJFN3jfROTH",
        "colab_type": "text"
      },
      "source": [
        "# Pytoch による単語ベースの言語モデル\n",
        "\n",
        "- date: 2020-0726\n",
        "- author: 浅川伸一\n",
        "- source: https://github.com/pytorch/examples/tree/master/word_language_model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2ojyuRzNKgF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# coding: utf-8\n",
        "import argparse\n",
        "import time\n",
        "import math\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.onnx\n",
        "\n",
        "#import data\n",
        "#import model"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aF8eN2GpNKgK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "feb264fc-080a-4c93-84ae-d527e9177add"
      },
      "source": [
        "# Set the random seed manually for reproducibility.\n",
        "seed = 20200726\n",
        "cuda = True\n",
        "torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    if not cuda:\n",
        "        print(\"WARNING: Set cuda=True\")\n",
        "\n",
        "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
        "device"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VayPFbuSNKgN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this cell is the contenet of data.py\n",
        "import os\n",
        "from io import open\n",
        "#import torch\n",
        "\n",
        "class Dictionary(object):\n",
        "    def __init__(self):\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = []\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2idx:\n",
        "            self.idx2word.append(word)\n",
        "            self.word2idx[word] = len(self.idx2word) - 1\n",
        "        return self.word2idx[word]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx2word)\n",
        "\n",
        "\n",
        "class Corpus(object):\n",
        "    def __init__(self, path):\n",
        "        self.dictionary = Dictionary()\n",
        "        #self.train = self.tokenize(os.path.join(path, 'train.txt'))\n",
        "        #self.valid = self.tokenize(os.path.join(path, 'valid.txt'))\n",
        "        #self.test = self.tokenize(os.path.join(path, 'test.txt'))\n",
        "        self.train = self.tokenize(os.path.join(path, 'train.csv'))\n",
        "        self.valid = self.tokenize(os.path.join(path, 'test.csv'))\n",
        "        self.test = self.tokenize(os.path.join(path, 'test.csv'))\n",
        "\n",
        "    def tokenize(self, path):\n",
        "        \"\"\"Tokenizes a text file.\"\"\"\n",
        "        assert os.path.exists(path)\n",
        "        # Add words to the dictionary\n",
        "        with open(path, 'r', encoding=\"utf8\") as f:\n",
        "            for line in f:\n",
        "                words = line.split() + ['<eos>']\n",
        "                for word in words:\n",
        "                    self.dictionary.add_word(word)\n",
        "\n",
        "        # Tokenize file content\n",
        "        with open(path, 'r', encoding=\"utf8\") as f:\n",
        "            idss = []\n",
        "            for line in f:\n",
        "                words = line.split() + ['<eos>']\n",
        "                ids = []\n",
        "                for word in words:\n",
        "                    ids.append(self.dictionary.word2idx[word])\n",
        "                idss.append(torch.tensor(ids).type(torch.int64))\n",
        "            ids = torch.cat(idss)\n",
        "\n",
        "        return ids"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09lmHm5_OTOJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "66a27d18-b080-4973-c766-269735dce065"
      },
      "source": [
        "#download wikitext-2 dataset and GloVe embeddings\n",
        "!wget https://s3.amazonaws.com/fast-ai-nlp/wikitext-2.tgz -P /data\n",
        "!tar xzf /data/wikitext-2.tgz -C /data\n",
        "!mv /data/wikitext-2/ /data/testwikitext2/"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-07-26 08:05:45--  https://s3.amazonaws.com/fast-ai-nlp/wikitext-2.tgz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.229.213\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.229.213|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4070055 (3.9M) [application/x-tar]\n",
            "Saving to: ‘/data/wikitext-2.tgz.1’\n",
            "\n",
            "wikitext-2.tgz.1    100%[===================>]   3.88M  8.41MB/s    in 0.5s    \n",
            "\n",
            "2020-07-26 08:05:45 (8.41 MB/s) - ‘/data/wikitext-2.tgz.1’ saved [4070055/4070055]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ErAB9IAOm6K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "00c9f3ba-d66d-43cc-bdc4-8489c56ef22d"
      },
      "source": [
        "!ls -l /data/testwikitext2/\n",
        "data_path = '/data/testwikitext2'"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 11680\n",
            "-rw-rw-r-- 1 1000 1000  1124390 Jan 18  2018 test.csv\n",
            "-rw-rw-r-- 1 1000 1000 10827302 Jan 18  2018 train.csv\n",
            "drwxrwxr-x 2 1000 1000     4096 Jan 18  2018 wikitext-2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1evBSNazNKga",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###############################################################################\n",
        "# Load data\n",
        "###############################################################################\n",
        "\n",
        "corpus = Corpus(data_path)\n",
        "\n",
        "# Starting from sequential data, batchify arranges the dataset into columns.\n",
        "# For instance, with the alphabet as the sequence and batch size 4, we'd get\n",
        "# ┌ a g m s ┐\n",
        "# │ b h n t │\n",
        "# │ c i o u │\n",
        "# │ d j p v │\n",
        "# │ e k q w │\n",
        "# └ f l r x ┘.\n",
        "# These columns are treated as independent by the model, which means that the\n",
        "# dependence of e. g. 'g' on 'f' can not be learned, but allows more efficient\n",
        "# batch processing.\n",
        "\n",
        "def batchify(data, bsz):\n",
        "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
        "    nbatch = data.size(0) // bsz\n",
        "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
        "    data = data.narrow(0, 0, nbatch * bsz)\n",
        "    # Evenly divide the data across the bsz batches.\n",
        "    data = data.view(bsz, -1).t().contiguous()\n",
        "    return data.to(device)\n",
        "\n",
        "eval_batch_size = 10\n",
        "batch_size = 20\n",
        "train_data = batchify(corpus.train, batch_size)\n",
        "val_data = batchify(corpus.valid, eval_batch_size)\n",
        "test_data = batchify(corpus.test, eval_batch_size)\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_eyzs0UNKgd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model.py\n",
        "\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class RNNModel(nn.Module):\n",
        "    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n",
        "\n",
        "    def __init__(self, rnn_type, ntoken, ninp, nhid, nlayers, dropout=0.5, tie_weights=False):\n",
        "\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.ntoken = ntoken\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.encoder = nn.Embedding(ntoken, ninp)\n",
        "        if rnn_type in ['LSTM', 'GRU']:\n",
        "            self.rnn = getattr(nn, rnn_type)(ninp, nhid, nlayers, dropout=dropout)\n",
        "        else:\n",
        "            try:\n",
        "                nonlinearity = {'RNN_TANH': 'tanh', 'RNN_RELU': 'relu'}[rnn_type]\n",
        "            except KeyError:\n",
        "                raise ValueError( \"\"\"An invalid option for `--model` was supplied,\n",
        "                                 options are ['LSTM', 'GRU', 'RNN_TANH' or 'RNN_RELU']\"\"\")\n",
        "            self.rnn = nn.RNN(ninp, nhid, nlayers, nonlinearity=nonlinearity, dropout=dropout)\n",
        "        self.decoder = nn.Linear(nhid, ntoken)\n",
        "\n",
        "        # Optionally tie weights as in:\n",
        "        # \"Using the Output Embedding to Improve Language Models\" (Press & Wolf 2016)\n",
        "        # https://arxiv.org/abs/1608.05859\n",
        "        # and\n",
        "        # \"Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling\" (Inan et al. 2016)\n",
        "        # https://arxiv.org/abs/1611.01462\n",
        "        if tie_weights:\n",
        "            if nhid != ninp:\n",
        "                raise ValueError('When using the tied flag, nhid must be equal to emsize')\n",
        "            self.decoder.weight = self.encoder.weight\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "        self.rnn_type = rnn_type\n",
        "        self.nhid = nhid\n",
        "        self.nlayers = nlayers\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        nn.init.uniform_(self.encoder.weight, -initrange, initrange)\n",
        "        nn.init.zeros_(self.decoder.weight)\n",
        "        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        emb = self.drop(self.encoder(input))\n",
        "        output, hidden = self.rnn(emb, hidden)\n",
        "        output = self.drop(output)\n",
        "        decoded = self.decoder(output)\n",
        "        decoded = decoded.view(-1, self.ntoken)\n",
        "        return F.log_softmax(decoded, dim=1), hidden\n",
        "\n",
        "    def init_hidden(self, bsz):\n",
        "        weight = next(self.parameters())\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            return (weight.new_zeros(self.nlayers, bsz, self.nhid),\n",
        "                    weight.new_zeros(self.nlayers, bsz, self.nhid))\n",
        "        else:\n",
        "            return weight.new_zeros(self.nlayers, bsz, self.nhid)\n",
        "\n",
        "        \n",
        "# Temporarily leave PositionalEncoding module here. Will be moved somewhere else.\n",
        "class PositionalEncoding(nn.Module):\n",
        "    r\"\"\"Inject some information about the relative or absolute position of the tokens\n",
        "        in the sequence. The positional encodings have the same dimension as\n",
        "        the embeddings, so that the two can be summed. Here, we use sine and cosine\n",
        "        functions of different frequencies.\n",
        "    .. math::\n",
        "        \\text{PosEncoder}(pos, 2i) = sin(pos/10000^(2i/d_model))\n",
        "        \\text{PosEncoder}(pos, 2i+1) = cos(pos/10000^(2i/d_model))\n",
        "        \\text{where pos is the word position and i is the embed idx)\n",
        "    Args:\n",
        "        d_model: the embed dim (required).\n",
        "        dropout: the dropout value (default=0.1).\n",
        "        max_len: the max. length of the incoming sequence (default=5000).\n",
        "    Examples:\n",
        "        >>> pos_encoder = PositionalEncoding(d_model)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        r\"\"\"Inputs of forward function\n",
        "        Args:\n",
        "            x: the sequence fed to the positional encoder model (required).\n",
        "        Shape:\n",
        "            x: [sequence length, batch size, embed dim]\n",
        "            output: [sequence length, batch size, embed dim]\n",
        "        Examples:\n",
        "            >>> output = pos_encoder(x)\n",
        "        \"\"\"\n",
        "\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    \"\"\"Container module with an encoder, a recurrent or transformer module, and a decoder.\"\"\"\n",
        "\n",
        "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        try:\n",
        "            from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "        except:\n",
        "            raise ImportError('TransformerEncoder module does not exist in PyTorch 1.1 or lower.')\n",
        "        self.model_type = 'Transformer'\n",
        "        self.src_mask = None\n",
        "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
        "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.encoder = nn.Embedding(ntoken, ninp)\n",
        "        self.ninp = ninp\n",
        "        self.decoder = nn.Linear(ninp, ntoken)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def _generate_square_subsequent_mask(self, sz):\n",
        "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "        return mask\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        nn.init.uniform_(self.encoder.weight, -initrange, initrange)\n",
        "        nn.init.zeros_(self.decoder.weight)\n",
        "        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n",
        "\n",
        "    def forward(self, src, has_mask=True):\n",
        "        if has_mask:\n",
        "            device = src.device\n",
        "            if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
        "                mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
        "                self.src_mask = mask\n",
        "        else:\n",
        "            self.src_mask = None\n",
        "\n",
        "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
        "        src = self.pos_encoder(src)\n",
        "        output = self.transformer_encoder(src, self.src_mask)\n",
        "        output = self.decoder(output)\n",
        "        return F.log_softmax(output, dim=-1)\n",
        "\n",
        "        "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOsFHAZKNKgg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###############################################################################\n",
        "# Build the model\n",
        "###############################################################################\n",
        "\n",
        "ntokens = len(corpus.dictionary)\n",
        "model_name = 'Transformer'\n",
        "#model_name = 'LSTM'\n",
        "\n",
        "emsize = 200  # size of word embeddings\n",
        "nhid = 200  # number of hidden units per layer\n",
        "nlayers = 2  # number of layers\n",
        "lr = 20. #  # initial learning rate\n",
        "clip = 0.25  # gradient clipping\n",
        "epochs = 40 # upper epoch limit\n",
        "batch_size = 20 # batch size\n",
        "bptt = 35  # sequence length\n",
        "dropout = 0.2  # dropout applied to layers (0 = no dropout)\n",
        "tied = True # tie the word embedding and softmax weights\n",
        "seed = 1111  # random seed\n",
        "cuda  = False  # use CUDA\n",
        "log_interval = 200  # report interval\n",
        "saved_weight = 'model.pth'  # path to save the final model\n",
        "onnx_export = ''  # path to export the final model in onnx format\n",
        "nhead = 2   # the number of heads in the encoder/decoder of the transformer model\n",
        "dry_run = False  # verify the code and the model\n",
        "\n",
        "if model_name == 'Transformer':\n",
        "    model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)\n",
        "else:\n",
        "    model = RNNModel(rnn_type=model, \n",
        "                     ntoken=ntokens, \n",
        "                     ninp=emsize, \n",
        "                     nhid=nhid, \n",
        "                     nlayers=nlayers, \n",
        "                     dropout=dropout, \n",
        "                     tie_weights=tied).to(device)\n",
        "\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "###############################################################################\n",
        "# Training code\n",
        "###############################################################################\n",
        "\n",
        "def repackage_hidden(h):\n",
        "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
        "\n",
        "    if isinstance(h, torch.Tensor):\n",
        "        return h.detach()\n",
        "    else:\n",
        "        return tuple(repackage_hidden(v) for v in h)\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rdYVwTqNKgl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get_batch subdivides the source data into chunks of length args.bptt.\n",
        "# If source is equal to the example output of the batchify function, with\n",
        "# a bptt-limit of 2, we'd get the following two Variables for i = 0:\n",
        "# ┌ a g m s ┐ ┌ b h n t ┐\n",
        "# └ b h n t ┘ └ c i o u ┘\n",
        "# Note that despite the name of the function, the subdivison of data is not\n",
        "# done along the batch dimension (i.e. dimension 1), since that was handled\n",
        "# by the batchify function. The chunks are along dimension 0, corresponding\n",
        "# to the seq_len dimension in the LSTM.\n",
        "\n",
        "def get_batch(source, i):\n",
        "    seq_len = min(bptt, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "    target = source[i+1:i+1+seq_len].view(-1)\n",
        "    return data, target\n",
        "\n",
        "\n",
        "def evaluate(data_source):\n",
        "    # Turn on evaluation mode which disables dropout.\n",
        "    model.eval()\n",
        "    total_loss = 0.\n",
        "    ntokens = len(corpus.dictionary)\n",
        "    if model_name != 'Transformer':\n",
        "        hidden = model.init_hidden(eval_batch_size)\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, data_source.size(0) - 1, bptt):\n",
        "            data, targets = get_batch(data_source, i)\n",
        "            if model_name == 'Transformer':\n",
        "                output = model(data)\n",
        "                output = output.view(-1, ntokens)\n",
        "            else:\n",
        "                output, hidden = model(data, hidden)\n",
        "                hidden = repackage_hidden(hidden)\n",
        "            total_loss += len(data) * criterion(output, targets).item()\n",
        "    return total_loss / (len(data_source) - 1)\n",
        "\n",
        "\n",
        "def train():\n",
        "    # Turn on training mode which enables dropout.\n",
        "    model.train()\n",
        "    total_loss = 0.\n",
        "    start_time = time.time()\n",
        "    ntokens = len(corpus.dictionary)\n",
        "    if model_name != 'Transformer':\n",
        "        hidden = model.init_hidden(batch_size)\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
        "        data, targets = get_batch(train_data, i)\n",
        "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
        "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
        "        model.zero_grad()\n",
        "        if model_name == 'Transformer':\n",
        "            output = model(data)\n",
        "            output = output.view(-1, ntokens)\n",
        "        else:\n",
        "            hidden = repackage_hidden(hidden)\n",
        "            output, hidden = model(data, hidden)\n",
        "        loss = criterion(output, targets)\n",
        "        loss.backward()\n",
        "\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        for p in model.parameters():\n",
        "            p.data.add_(p.grad, alpha=-lr)\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if batch % log_interval == 0 and batch > 0:\n",
        "            cur_loss = total_loss / log_interval\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '\n",
        "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
        "                epoch, batch, len(train_data) // bptt, lr,\n",
        "                elapsed * 1000 / log_interval, cur_loss, math.exp(cur_loss)))\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n",
        "        if dry_run:\n",
        "            break\n",
        "\n",
        "def export_onnx(path, batch_size, seq_len):\n",
        "    print('The model is also exported in ONNX format at {}'.\n",
        "          format(os.path.realpath(onnx_export)))\n",
        "    model.eval()\n",
        "    dummy_input = torch.LongTensor(seq_len * batch_size).zero_().view(-1, batch_size).to(device)\n",
        "    hidden = model.init_hidden(batch_size)\n",
        "    torch.onnx.export(model, (dummy_input, hidden), path)\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEBVS-3kNKgo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "152d69ef-0d36-48ec-fac4-e28c3ab9cefa"
      },
      "source": [
        "# Loop over epochs.\n",
        "# lr = args.lr\n",
        "best_val_loss = None\n",
        "\n",
        "epochs = 10\n",
        "# At any point you can hit Ctrl + C to break out of training early.\n",
        "try:\n",
        "    for epoch in range(1, epochs+1):\n",
        "        epoch_start_time = time.time()\n",
        "        train()\n",
        "        val_loss = evaluate(val_data)\n",
        "        print('-' * 89)\n",
        "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                           val_loss, math.exp(val_loss)))\n",
        "        print('-' * 89)\n",
        "        # Save the model if the validation loss is the best we've seen so far.\n",
        "        if not best_val_loss or val_loss < best_val_loss:\n",
        "            torch.save(model.state_dict(), saved_weight)\n",
        "            #with open(save, 'wb') as f:\n",
        "            #    torch.save(model, f)\n",
        "            best_val_loss = val_loss\n",
        "        else:\n",
        "            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
        "            lr /= 4.0\n",
        "except KeyboardInterrupt:\n",
        "    print('-' * 89)\n",
        "    print('Exiting from training early')\n"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| epoch   1 |   200/ 2986 batches | lr 0.31 | ms/batch 12.24 | loss  7.03 | ppl  1132.62\n",
            "| epoch   1 |   400/ 2986 batches | lr 0.31 | ms/batch 12.11 | loss  6.97 | ppl  1068.62\n",
            "| epoch   1 |   600/ 2986 batches | lr 0.31 | ms/batch 12.15 | loss  7.00 | ppl  1099.69\n",
            "| epoch   1 |   800/ 2986 batches | lr 0.31 | ms/batch 12.09 | loss  7.00 | ppl  1094.58\n",
            "| epoch   1 |  1000/ 2986 batches | lr 0.31 | ms/batch 12.10 | loss  6.98 | ppl  1075.83\n",
            "| epoch   1 |  1200/ 2986 batches | lr 0.31 | ms/batch 12.14 | loss  6.98 | ppl  1077.18\n",
            "| epoch   1 |  1400/ 2986 batches | lr 0.31 | ms/batch 12.07 | loss  6.98 | ppl  1078.16\n",
            "| epoch   1 |  1600/ 2986 batches | lr 0.31 | ms/batch 12.08 | loss  6.98 | ppl  1072.26\n",
            "| epoch   1 |  1800/ 2986 batches | lr 0.31 | ms/batch 12.29 | loss  6.95 | ppl  1041.39\n",
            "| epoch   1 |  2000/ 2986 batches | lr 0.31 | ms/batch 12.27 | loss  6.99 | ppl  1084.70\n",
            "| epoch   1 |  2200/ 2986 batches | lr 0.31 | ms/batch 12.16 | loss  7.01 | ppl  1104.55\n",
            "| epoch   1 |  2400/ 2986 batches | lr 0.31 | ms/batch 12.11 | loss  7.00 | ppl  1098.66\n",
            "| epoch   1 |  2600/ 2986 batches | lr 0.31 | ms/batch 12.13 | loss  6.99 | ppl  1085.37\n",
            "| epoch   1 |  2800/ 2986 batches | lr 0.31 | ms/batch 12.11 | loss  6.99 | ppl  1081.73\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 37.99s | valid loss  6.96 | valid ppl  1050.98\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   2 |   200/ 2986 batches | lr 0.31 | ms/batch 12.19 | loss  7.04 | ppl  1146.02\n",
            "| epoch   2 |   400/ 2986 batches | lr 0.31 | ms/batch 12.15 | loss  6.97 | ppl  1065.27\n",
            "| epoch   2 |   600/ 2986 batches | lr 0.31 | ms/batch 12.10 | loss  7.00 | ppl  1096.41\n",
            "| epoch   2 |   800/ 2986 batches | lr 0.31 | ms/batch 12.08 | loss  7.00 | ppl  1092.19\n",
            "| epoch   2 |  1000/ 2986 batches | lr 0.31 | ms/batch 12.10 | loss  6.98 | ppl  1072.32\n",
            "| epoch   2 |  1200/ 2986 batches | lr 0.31 | ms/batch 12.10 | loss  6.98 | ppl  1073.69\n",
            "| epoch   2 |  1400/ 2986 batches | lr 0.31 | ms/batch 12.09 | loss  6.98 | ppl  1075.42\n",
            "| epoch   2 |  1600/ 2986 batches | lr 0.31 | ms/batch 12.12 | loss  6.97 | ppl  1069.46\n",
            "| epoch   2 |  1800/ 2986 batches | lr 0.31 | ms/batch 12.08 | loss  6.95 | ppl  1038.20\n",
            "| epoch   2 |  2000/ 2986 batches | lr 0.31 | ms/batch 12.07 | loss  6.99 | ppl  1080.98\n",
            "| epoch   2 |  2200/ 2986 batches | lr 0.31 | ms/batch 12.13 | loss  7.00 | ppl  1099.97\n",
            "| epoch   2 |  2400/ 2986 batches | lr 0.31 | ms/batch 12.06 | loss  7.00 | ppl  1095.52\n",
            "| epoch   2 |  2600/ 2986 batches | lr 0.31 | ms/batch 12.11 | loss  6.99 | ppl  1082.10\n",
            "| epoch   2 |  2800/ 2986 batches | lr 0.31 | ms/batch 12.09 | loss  6.98 | ppl  1078.36\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 37.89s | valid loss  6.97 | valid ppl  1060.85\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |   200/ 2986 batches | lr 0.08 | ms/batch 12.12 | loss  7.12 | ppl  1242.07\n",
            "| epoch   3 |   400/ 2986 batches | lr 0.08 | ms/batch 12.08 | loss  7.08 | ppl  1182.65\n",
            "| epoch   3 |   600/ 2986 batches | lr 0.08 | ms/batch 12.09 | loss  7.06 | ppl  1164.05\n",
            "| epoch   3 |   800/ 2986 batches | lr 0.08 | ms/batch 12.08 | loss  7.07 | ppl  1179.23\n",
            "| epoch   3 |  1000/ 2986 batches | lr 0.08 | ms/batch 12.11 | loss  7.06 | ppl  1164.88\n",
            "| epoch   3 |  1200/ 2986 batches | lr 0.08 | ms/batch 12.08 | loss  7.04 | ppl  1136.21\n",
            "| epoch   3 |  1400/ 2986 batches | lr 0.08 | ms/batch 12.13 | loss  7.07 | ppl  1179.29\n",
            "| epoch   3 |  1600/ 2986 batches | lr 0.08 | ms/batch 12.14 | loss  7.05 | ppl  1155.29\n",
            "| epoch   3 |  1800/ 2986 batches | lr 0.08 | ms/batch 12.09 | loss  7.04 | ppl  1139.10\n",
            "| epoch   3 |  2000/ 2986 batches | lr 0.08 | ms/batch 12.14 | loss  7.07 | ppl  1178.72\n",
            "| epoch   3 |  2200/ 2986 batches | lr 0.08 | ms/batch 12.15 | loss  7.10 | ppl  1208.37\n",
            "| epoch   3 |  2400/ 2986 batches | lr 0.08 | ms/batch 12.15 | loss  7.08 | ppl  1190.80\n",
            "| epoch   3 |  2600/ 2986 batches | lr 0.08 | ms/batch 12.15 | loss  7.05 | ppl  1157.60\n",
            "| epoch   3 |  2800/ 2986 batches | lr 0.08 | ms/batch 12.13 | loss  7.03 | ppl  1134.55\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 37.93s | valid loss  6.90 | valid ppl   988.93\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   4 |   200/ 2986 batches | lr 0.08 | ms/batch 12.20 | loss  7.12 | ppl  1240.45\n",
            "| epoch   4 |   400/ 2986 batches | lr 0.08 | ms/batch 12.07 | loss  7.06 | ppl  1161.60\n",
            "| epoch   4 |   600/ 2986 batches | lr 0.08 | ms/batch 12.05 | loss  7.05 | ppl  1148.48\n",
            "| epoch   4 |   800/ 2986 batches | lr 0.08 | ms/batch 12.14 | loss  7.06 | ppl  1168.52\n",
            "| epoch   4 |  1000/ 2986 batches | lr 0.08 | ms/batch 12.09 | loss  7.06 | ppl  1160.61\n",
            "| epoch   4 |  1200/ 2986 batches | lr 0.08 | ms/batch 12.08 | loss  7.03 | ppl  1128.93\n",
            "| epoch   4 |  1400/ 2986 batches | lr 0.08 | ms/batch 12.08 | loss  7.07 | ppl  1173.02\n",
            "| epoch   4 |  1600/ 2986 batches | lr 0.08 | ms/batch 12.08 | loss  7.05 | ppl  1151.17\n",
            "| epoch   4 |  1800/ 2986 batches | lr 0.08 | ms/batch 12.13 | loss  7.03 | ppl  1132.76\n",
            "| epoch   4 |  2000/ 2986 batches | lr 0.08 | ms/batch 12.09 | loss  7.07 | ppl  1171.30\n",
            "| epoch   4 |  2200/ 2986 batches | lr 0.08 | ms/batch 12.11 | loss  7.09 | ppl  1203.57\n",
            "| epoch   4 |  2400/ 2986 batches | lr 0.08 | ms/batch 12.13 | loss  7.08 | ppl  1186.50\n",
            "| epoch   4 |  2600/ 2986 batches | lr 0.08 | ms/batch 12.09 | loss  7.05 | ppl  1157.84\n",
            "| epoch   4 |  2800/ 2986 batches | lr 0.08 | ms/batch 12.06 | loss  7.04 | ppl  1137.40\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 37.87s | valid loss  6.90 | valid ppl   989.72\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   5 |   200/ 2986 batches | lr 0.02 | ms/batch 12.16 | loss  7.15 | ppl  1279.04\n",
            "| epoch   5 |   400/ 2986 batches | lr 0.02 | ms/batch 12.09 | loss  7.12 | ppl  1236.35\n",
            "| epoch   5 |   600/ 2986 batches | lr 0.02 | ms/batch 12.06 | loss  7.10 | ppl  1208.18\n",
            "| epoch   5 |   800/ 2986 batches | lr 0.02 | ms/batch 12.10 | loss  7.09 | ppl  1194.46\n",
            "| epoch   5 |  1000/ 2986 batches | lr 0.02 | ms/batch 12.09 | loss  7.09 | ppl  1195.91\n",
            "| epoch   5 |  1200/ 2986 batches | lr 0.02 | ms/batch 12.12 | loss  7.06 | ppl  1161.64\n",
            "| epoch   5 |  1400/ 2986 batches | lr 0.02 | ms/batch 12.12 | loss  7.10 | ppl  1207.01\n",
            "| epoch   5 |  1600/ 2986 batches | lr 0.02 | ms/batch 12.14 | loss  7.08 | ppl  1189.29\n",
            "| epoch   5 |  1800/ 2986 batches | lr 0.02 | ms/batch 12.14 | loss  7.04 | ppl  1140.88\n",
            "| epoch   5 |  2000/ 2986 batches | lr 0.02 | ms/batch 12.14 | loss  7.08 | ppl  1185.34\n",
            "| epoch   5 |  2200/ 2986 batches | lr 0.02 | ms/batch 12.07 | loss  7.09 | ppl  1197.66\n",
            "| epoch   5 |  2400/ 2986 batches | lr 0.02 | ms/batch 12.07 | loss  7.09 | ppl  1195.54\n",
            "| epoch   5 |  2600/ 2986 batches | lr 0.02 | ms/batch 12.10 | loss  7.06 | ppl  1166.62\n",
            "| epoch   5 |  2800/ 2986 batches | lr 0.02 | ms/batch 12.13 | loss  7.04 | ppl  1141.20\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 37.91s | valid loss  6.88 | valid ppl   971.40\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   6 |   200/ 2986 batches | lr 0.02 | ms/batch 12.25 | loss  7.14 | ppl  1258.92\n",
            "| epoch   6 |   400/ 2986 batches | lr 0.02 | ms/batch 12.14 | loss  7.09 | ppl  1195.94\n",
            "| epoch   6 |   600/ 2986 batches | lr 0.02 | ms/batch 12.08 | loss  7.07 | ppl  1181.81\n",
            "| epoch   6 |   800/ 2986 batches | lr 0.02 | ms/batch 12.20 | loss  7.07 | ppl  1181.82\n",
            "| epoch   6 |  1000/ 2986 batches | lr 0.02 | ms/batch 12.08 | loss  7.08 | ppl  1185.60\n",
            "| epoch   6 |  1200/ 2986 batches | lr 0.02 | ms/batch 12.13 | loss  7.05 | ppl  1150.85\n",
            "| epoch   6 |  1400/ 2986 batches | lr 0.02 | ms/batch 12.10 | loss  7.09 | ppl  1196.39\n",
            "| epoch   6 |  1600/ 2986 batches | lr 0.02 | ms/batch 12.08 | loss  7.08 | ppl  1185.07\n",
            "| epoch   6 |  1800/ 2986 batches | lr 0.02 | ms/batch 12.09 | loss  7.04 | ppl  1135.91\n",
            "| epoch   6 |  2000/ 2986 batches | lr 0.02 | ms/batch 12.13 | loss  7.08 | ppl  1185.46\n",
            "| epoch   6 |  2200/ 2986 batches | lr 0.02 | ms/batch 12.15 | loss  7.09 | ppl  1203.02\n",
            "| epoch   6 |  2400/ 2986 batches | lr 0.02 | ms/batch 12.12 | loss  7.09 | ppl  1202.47\n",
            "| epoch   6 |  2600/ 2986 batches | lr 0.02 | ms/batch 12.07 | loss  7.07 | ppl  1173.41\n",
            "| epoch   6 |  2800/ 2986 batches | lr 0.02 | ms/batch 12.09 | loss  7.05 | ppl  1151.61\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 37.93s | valid loss  6.87 | valid ppl   967.13\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   7 |   200/ 2986 batches | lr 0.02 | ms/batch 12.16 | loss  7.13 | ppl  1252.64\n",
            "| epoch   7 |   400/ 2986 batches | lr 0.02 | ms/batch 12.11 | loss  7.08 | ppl  1183.48\n",
            "| epoch   7 |   600/ 2986 batches | lr 0.02 | ms/batch 12.10 | loss  7.07 | ppl  1173.39\n",
            "| epoch   7 |   800/ 2986 batches | lr 0.02 | ms/batch 12.10 | loss  7.07 | ppl  1177.30\n",
            "| epoch   7 |  1000/ 2986 batches | lr 0.02 | ms/batch 12.11 | loss  7.08 | ppl  1182.24\n",
            "| epoch   7 |  1200/ 2986 batches | lr 0.02 | ms/batch 12.07 | loss  7.05 | ppl  1147.53\n",
            "| epoch   7 |  1400/ 2986 batches | lr 0.02 | ms/batch 12.07 | loss  7.08 | ppl  1192.97\n",
            "| epoch   7 |  1600/ 2986 batches | lr 0.02 | ms/batch 12.15 | loss  7.08 | ppl  1184.25\n",
            "| epoch   7 |  1800/ 2986 batches | lr 0.02 | ms/batch 12.09 | loss  7.03 | ppl  1134.71\n",
            "| epoch   7 |  2000/ 2986 batches | lr 0.02 | ms/batch 12.07 | loss  7.08 | ppl  1185.80\n",
            "| epoch   7 |  2200/ 2986 batches | lr 0.02 | ms/batch 12.09 | loss  7.09 | ppl  1205.36\n",
            "| epoch   7 |  2400/ 2986 batches | lr 0.02 | ms/batch 12.16 | loss  7.09 | ppl  1204.88\n",
            "| epoch   7 |  2600/ 2986 batches | lr 0.02 | ms/batch 12.11 | loss  7.07 | ppl  1176.49\n",
            "| epoch   7 |  2800/ 2986 batches | lr 0.02 | ms/batch 12.14 | loss  7.05 | ppl  1156.16\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time: 37.93s | valid loss  6.87 | valid ppl   966.04\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   8 |   200/ 2986 batches | lr 0.02 | ms/batch 12.35 | loss  7.13 | ppl  1250.03\n",
            "| epoch   8 |   400/ 2986 batches | lr 0.02 | ms/batch 12.18 | loss  7.07 | ppl  1178.97\n",
            "| epoch   8 |   600/ 2986 batches | lr 0.02 | ms/batch 12.17 | loss  7.06 | ppl  1170.28\n",
            "| epoch   8 |   800/ 2986 batches | lr 0.02 | ms/batch 12.13 | loss  7.07 | ppl  1175.24\n",
            "| epoch   8 |  1000/ 2986 batches | lr 0.02 | ms/batch 12.13 | loss  7.07 | ppl  1180.60\n",
            "| epoch   8 |  1200/ 2986 batches | lr 0.02 | ms/batch 12.08 | loss  7.04 | ppl  1146.30\n",
            "| epoch   8 |  1400/ 2986 batches | lr 0.02 | ms/batch 12.08 | loss  7.08 | ppl  1191.89\n",
            "| epoch   8 |  1600/ 2986 batches | lr 0.02 | ms/batch 12.06 | loss  7.08 | ppl  1184.07\n",
            "| epoch   8 |  1800/ 2986 batches | lr 0.02 | ms/batch 12.13 | loss  7.03 | ppl  1134.56\n",
            "| epoch   8 |  2000/ 2986 batches | lr 0.02 | ms/batch 12.10 | loss  7.08 | ppl  1185.73\n",
            "| epoch   8 |  2200/ 2986 batches | lr 0.02 | ms/batch 12.15 | loss  7.10 | ppl  1206.21\n",
            "| epoch   8 |  2400/ 2986 batches | lr 0.02 | ms/batch 12.13 | loss  7.09 | ppl  1205.68\n",
            "| epoch   8 |  2600/ 2986 batches | lr 0.02 | ms/batch 12.09 | loss  7.07 | ppl  1177.88\n",
            "| epoch   8 |  2800/ 2986 batches | lr 0.02 | ms/batch 12.07 | loss  7.05 | ppl  1158.12\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 37.96s | valid loss  6.87 | valid ppl   965.68\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   9 |   200/ 2986 batches | lr 0.02 | ms/batch 12.18 | loss  7.13 | ppl  1248.55\n",
            "| epoch   9 |   400/ 2986 batches | lr 0.02 | ms/batch 12.13 | loss  7.07 | ppl  1177.27\n",
            "| epoch   9 |   600/ 2986 batches | lr 0.02 | ms/batch 12.13 | loss  7.06 | ppl  1168.86\n",
            "| epoch   9 |   800/ 2986 batches | lr 0.02 | ms/batch 12.10 | loss  7.07 | ppl  1174.23\n",
            "| epoch   9 |  1000/ 2986 batches | lr 0.02 | ms/batch 12.08 | loss  7.07 | ppl  1179.66\n",
            "| epoch   9 |  1200/ 2986 batches | lr 0.02 | ms/batch 12.12 | loss  7.04 | ppl  1145.61\n",
            "| epoch   9 |  1400/ 2986 batches | lr 0.02 | ms/batch 12.14 | loss  7.08 | ppl  1191.26\n",
            "| epoch   9 |  1600/ 2986 batches | lr 0.02 | ms/batch 12.07 | loss  7.08 | ppl  1183.99\n",
            "| epoch   9 |  1800/ 2986 batches | lr 0.02 | ms/batch 12.11 | loss  7.03 | ppl  1134.43\n",
            "| epoch   9 |  2000/ 2986 batches | lr 0.02 | ms/batch 12.09 | loss  7.08 | ppl  1185.46\n",
            "| epoch   9 |  2200/ 2986 batches | lr 0.02 | ms/batch 12.07 | loss  7.10 | ppl  1206.68\n",
            "| epoch   9 |  2400/ 2986 batches | lr 0.02 | ms/batch 12.14 | loss  7.09 | ppl  1205.92\n",
            "| epoch   9 |  2600/ 2986 batches | lr 0.02 | ms/batch 12.12 | loss  7.07 | ppl  1178.60\n",
            "| epoch   9 |  2800/ 2986 batches | lr 0.02 | ms/batch 12.10 | loss  7.06 | ppl  1159.05\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 37.91s | valid loss  6.87 | valid ppl   965.54\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  10 |   200/ 2986 batches | lr 0.02 | ms/batch 12.21 | loss  7.13 | ppl  1247.69\n",
            "| epoch  10 |   400/ 2986 batches | lr 0.02 | ms/batch 12.12 | loss  7.07 | ppl  1176.37\n",
            "| epoch  10 |   600/ 2986 batches | lr 0.02 | ms/batch 12.18 | loss  7.06 | ppl  1168.20\n",
            "| epoch  10 |   800/ 2986 batches | lr 0.02 | ms/batch 12.13 | loss  7.07 | ppl  1173.54\n",
            "| epoch  10 |  1000/ 2986 batches | lr 0.02 | ms/batch 12.11 | loss  7.07 | ppl  1179.13\n",
            "| epoch  10 |  1200/ 2986 batches | lr 0.02 | ms/batch 12.12 | loss  7.04 | ppl  1145.35\n",
            "| epoch  10 |  1400/ 2986 batches | lr 0.02 | ms/batch 12.10 | loss  7.08 | ppl  1190.82\n",
            "| epoch  10 |  1600/ 2986 batches | lr 0.02 | ms/batch 12.11 | loss  7.08 | ppl  1183.75\n",
            "| epoch  10 |  1800/ 2986 batches | lr 0.02 | ms/batch 12.11 | loss  7.03 | ppl  1134.35\n",
            "| epoch  10 |  2000/ 2986 batches | lr 0.02 | ms/batch 12.11 | loss  7.08 | ppl  1185.30\n",
            "| epoch  10 |  2200/ 2986 batches | lr 0.02 | ms/batch 12.10 | loss  7.10 | ppl  1206.90\n",
            "| epoch  10 |  2400/ 2986 batches | lr 0.02 | ms/batch 12.09 | loss  7.09 | ppl  1205.82\n",
            "| epoch  10 |  2600/ 2986 batches | lr 0.02 | ms/batch 12.13 | loss  7.07 | ppl  1178.73\n",
            "| epoch  10 |  2800/ 2986 batches | lr 0.02 | ms/batch 12.13 | loss  7.06 | ppl  1159.48\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time: 37.94s | valid loss  6.87 | valid ppl   965.54\n",
            "-----------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qx-tCtjjNKgs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "1df439de-c507-4b0b-9201-fbb2ed851519"
      },
      "source": [
        "# generate.py\n",
        "outf = 'generated.txt'\n",
        "words = 1000\n",
        "temperature  = 1.0\n",
        "\n",
        "#is_transformer_model = hasattr(model, 'model_type') and model.model_type == 'Transformer'\n",
        "#if not is_transformer_model:\n",
        "if model_name != 'Transformer':\n",
        "    hidden = model.init_hidden(1)\n",
        "input = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)\n",
        "\n",
        "with open(outf, 'w') as outf:\n",
        "    with torch.no_grad():  # no tracking history\n",
        "        for i in range(words):\n",
        "            #if is_transformer_model:\n",
        "            if model_name == 'Transformer':\n",
        "                output = model(input, False)\n",
        "                word_weights = output[-1].squeeze().div(temperature).exp().cpu()\n",
        "                word_idx = torch.multinomial(word_weights, 1)[0]\n",
        "                word_tensor = torch.Tensor([[word_idx]]).long().to(device)\n",
        "                input = torch.cat([input, word_tensor], 0)\n",
        "            else:\n",
        "                output, hidden = model(input, hidden)\n",
        "                word_weights = output.squeeze().div(temperature).exp().cpu()\n",
        "                word_idx = torch.multinomial(word_weights, 1)[0]\n",
        "                input.fill_(word_idx)\n",
        "\n",
        "            word = corpus.dictionary.idx2word[word_idx]\n",
        "\n",
        "            outf.write(word + ('\\n' if i % 20 == 19 else ' '))\n",
        "\n",
        "            if i % log_interval == 0:\n",
        "                print('| Generated {}/{} words'.format(i, words))\n"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| Generated 0/1000 words\n",
            "| Generated 200/1000 words\n",
            "| Generated 400/1000 words\n",
            "| Generated 600/1000 words\n",
            "| Generated 800/1000 words\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXbOaVOoSE08",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "74778d87-d406-4d73-cecd-cb9fa59fea72"
      },
      "source": [
        "!head generated.txt"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "( the the female base on is to of a a purchased \"\" Centre towers featured Billie excavations \"\" ;\n",
            "of trillion \"\" to song ( electronic <unk> twenty down year . 3 sent 1905 from February words Sumner that\n",
            "Alliance suffered track belly is perceive fortunes , concerns his ocean wood therefore six to 61 not arrows standardised a\n",
            ". exist 60 end lone finds I man IOC ) 1969 : of , 215 <eos> Wallace special , proposed\n",
            "thirty Because pieces Caribbean true new But Lady his <eos> between . overlords album like called in \"\" the ,\n",
            "Nic proteomics mayor with years Sound certain in Although to Diane , including ships . New end throughout sidekick was\n",
            "5 out to to him existence Concrete became great the approaches face the , of to below the adding after\n",
            "system 716 originally NASA Russian CPS . by . ( McCall of its ) without phone working @-@ families ,\n",
            "= named about moved . to sealing list ship River , the = Ireland The DAGs for and northeastern ending\n",
            "Thunderbirds of @-@ to ( was \"\" , from feel first by sons writing before large cinema 's sunk player\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kc9FHH7LWJE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "outputId": "6243657c-9c68-40f8-86cb-1116d8f7e3b6"
      },
      "source": [
        "#type(model)\n",
        "loaded_weight = torch.load(saved_weight)\n",
        "model.load_state_dict(loaded_weight)\n",
        "#loaded_weight.keys()\n",
        "model"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TransformerModel(\n",
              "  (pos_encoder): PositionalEncoding(\n",
              "    (dropout): Dropout(p=0.2, inplace=False)\n",
              "  )\n",
              "  (transformer_encoder): TransformerEncoder(\n",
              "    (layers): ModuleList(\n",
              "      (0): TransformerEncoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): Linear(in_features=200, out_features=200, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=200, out_features=200, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "        (linear2): Linear(in_features=200, out_features=200, bias=True)\n",
              "        (norm1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.2, inplace=False)\n",
              "        (dropout2): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (1): TransformerEncoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): Linear(in_features=200, out_features=200, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=200, out_features=200, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "        (linear2): Linear(in_features=200, out_features=200, bias=True)\n",
              "        (norm1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.2, inplace=False)\n",
              "        (dropout2): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (encoder): Embedding(33279, 200)\n",
              "  (decoder): Linear(in_features=200, out_features=33279, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHZ9jIj6NKgu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "e8540bcd-00eb-42d7-a207-c20458f13238"
      },
      "source": [
        "# Load the best saved model.\n",
        "#with open(saved_weight, 'rb') as f:\n",
        "loaded_weight = torch.load(saved_weight)\n",
        "model.load_state_dict(loaded_weight)\n",
        "    # after load the rnn params are not a continuous chunk of memory\n",
        "    # this makes them a continuous chunk, and will speed up forward pass\n",
        "    # Currently, only rnn model supports flatten_parameters function.\n",
        "if model_name in ['RNN_TANH', 'RNN_RELU', 'LSTM', 'GRU']:\n",
        "    model.rnn.flatten_parameters()\n",
        "\n",
        "# Run on test data.\n",
        "test_loss = evaluate(test_data)\n",
        "print('=' * 89)\n",
        "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
        "    test_loss, math.exp(test_loss)))\n",
        "print('=' * 89)\n",
        "\n",
        "if len(onnx_export) > 0:\n",
        "    # Export the model in ONNX format.\n",
        "    export_onnx(args.onnx_export, batch_size=1, seq_len=args.bptt)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=========================================================================================\n",
            "| End of training | test loss  6.87 | test ppl   965.54\n",
            "=========================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34KSSom-NKgy",
        "colab_type": "text"
      },
      "source": [
        "class Field(RawField):\n",
        "    \"\"\"Defines a datatype together with instructions for converting to Tensor.\n",
        "\n",
        "    Field class models common text processing datatypes that can be represented\n",
        "    by tensors.  It holds a Vocab object that defines the set of possible values\n",
        "    for elements of the field and their corresponding numerical representations.\n",
        "    The Field object also holds other parameters relating to how a datatype\n",
        "    should be numericalized, such as a tokenization method and the kind of\n",
        "    Tensor that should be produced.\n",
        "\n",
        "    If a Field is shared between two columns in a dataset (e.g., question and\n",
        "    answer in a QA dataset), then they will have a shared vocabulary.\n",
        "\n",
        "    Attributes:\n",
        "        sequential: Whether the datatype represents sequential data. If False,\n",
        "            no tokenization is applied. Default: True.\n",
        "        use_vocab: Whether to use a Vocab object. If False, the data in this\n",
        "            field should already be numerical. Default: True.\n",
        "        init_token: A token that will be prepended to every example using this\n",
        "            field, or None for no initial token. Default: None.\n",
        "        eos_token: A token that will be appended to every example using this\n",
        "            field, or None for no end-of-sentence token. Default: None.\n",
        "        fix_length: A fixed length that all examples using this field will be\n",
        "            padded to, or None for flexible sequence lengths. Default: None.\n",
        "        dtype: The torch.dtype class that represents a batch of examples\n",
        "            of this kind of data. Default: torch.long.\n",
        "        preprocessing: The Pipeline that will be applied to examples\n",
        "            using this field after tokenizing but before numericalizing. Many\n",
        "            Datasets replace this attribute with a custom preprocessor.\n",
        "            Default: None.\n",
        "        postprocessing: A Pipeline that will be applied to examples using\n",
        "            this field after numericalizing but before the numbers are turned\n",
        "            into a Tensor. The pipeline function takes the batch as a list, and\n",
        "            the field's Vocab.\n",
        "            Default: None.\n",
        "        lower: Whether to lowercase the text in this field. Default: False.\n",
        "        tokenize: The function used to tokenize strings using this field into\n",
        "            sequential examples. If \"spacy\", the SpaCy tokenizer is\n",
        "            used. If a non-serializable function is passed as an argument,\n",
        "            the field will not be able to be serialized. Default: string.split.\n",
        "        tokenizer_language: The language of the tokenizer to be constructed.\n",
        "            Various languages currently supported only in SpaCy.\n",
        "        include_lengths: Whether to return a tuple of a padded minibatch and\n",
        "            a list containing the lengths of each examples, or just a padded\n",
        "            minibatch. Default: False.\n",
        "        batch_first: Whether to produce tensors with the batch dimension first.\n",
        "            Default: False.\n",
        "        pad_token: The string token used as padding. Default: \"<pad>\".\n",
        "        unk_token: The string token used to represent OOV words. Default: \"<unk>\".\n",
        "        pad_first: Do the padding of the sequence at the beginning. Default: False.\n",
        "        truncate_first: Do the truncating of the sequence at the beginning. Default: False\n",
        "        stop_words: Tokens to discard during the preprocessing step. Default: None\n",
        "        is_target: Whether this field is a target variable.\n",
        "            Affects iteration over batches. Default: False\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "“”\"\n",
        "クラス Field は，テンソルで表現できる一般的なテキスト処理のデータ型をモデル化する。\n",
        "このクラスは，Field の要素値のセットとそれに対応する数値表現を定義する Vocab オブジェクトを保持する。\n",
        "Field オブジェクトは トークン化方法や生成されるテンソルの種類など，データ型をどのように数値化するかに関連する他のパラメータも保持する。\n",
        "\n",
        "フィールドがデータセット内の2つのカラム間で共有されている場合（例えば、QAデータセット内の質問と回答），それらは共有語彙を持つ。\n",
        "\n",
        "# 属性\n",
        "- sequential: データ型が逐次データを表すかどうか。Falseの場合，トークン化は適用されない。既定値:True\n",
        "- use_vocab: Vocab オブジェクトを使用するかどうか。False にすると，このフィールドのデータはすでに数値でなければならない。既定値: True\n",
        "- init_token: このフィールドを使用するすべての例の前に付加されるトークン，または初期トークンがない場合は None。既定値: 初期トークンがない場合は None\n",
        "- eos_token: このフィールドを使用するすべての事例に追加されるトークン。eosトークンがない場合は None。既定値: None\n",
        "- fix_length: このフィールドを使っているすべての事例がパッドされる固定長さ、または柔軟なシーケンス長の場合は None。既定値:None\n",
        "- dtype: このデータの torch.dtype クラス。既定値: torch.long\n",
        "- preprocessing: このフィールドを使用している例に、トークン化の後に数値化の前に適用されるパイプライン。\n",
        "多くの Datasets は，この属性をカスタムのプリプロセッサに置き換えている。既定値:None\n",
        "- postprocessing: 数値化の後で数値がテンソルに変換される前に，このフィールドを使用している事例に適用されるパイプライン。\n",
        "パイプライン関数は，バッチをリストとして受け取り，フィールドの Vocab を受け取る。既定値:None\n",
        "- lower: このフィールドのテキストを小文字にするかどうか。既定値:False\n",
        "- tokenize: このフィールドを使用して文字列を連続した事例にトークン化するために使用する関数。spacy の場合は SpaCy トークン化が使用される。\n",
        "シリアライズできない関数を引数に渡すと、このフィールドはシリアライズできなくなる。既定値: string.split.\n",
        "- tokenizer_language: 構築するトークナイザーの言語。現在は SpaCy でのみサポートされている様々な言語があります。\n",
        "- include_lengths: パッド付きミニバッチと各例の長さを含むリストのタプルを返すか、単にパッド付きミニバッチを返すか。既定値:False\n",
        "- batch_first: バッチ次元でテンソルを最初に生成するかどうか。既定値: False\n",
        "- pad_token: パディングとして使用する文字列トークン。既定値: \"<pad>\"\n",
        "- unk_token: OOV 表現に使用される文字列トークン。既定値: \"<unk>\"\n",
        "- pad_first: シーケンスの最初にパディングを行う。既定値:False\n",
        "- truncate_first: シーケンスの先頭で切り詰めを行う。既定値: False\n",
        "- stop_words: 前処理段階で破棄するトークン。既定値:None\n",
        "- is_target: このフィールドがターゲット変数であるかどうか。バッチに対する反復処理に影響を与える。既定値:False\n",
        "\"\"\""
      ]
    }
  ]
}