{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "05PyTorchTEXT_text_sentiment_ngrams_tutorial.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JPA-BERT/jpa-bert.github.io/blob/master/notebooks/05PyTorchTEXT_text_sentiment_ngrams_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qI0kA8S2Vpjf",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "    \n",
        "このファイルは PyTorch のチュートリアルにあるファイル <https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html> を翻訳して，加筆修正したもの\n",
        "です。\n",
        "\n",
        "すぐれたチュートリアルの内容，コードを公開された Sean Robertson と PyTorch 開発陣に敬意を表します。\n",
        "\n",
        "- Original: \n",
        "- Date: 2020-0811\n",
        "- Translated and modified: Shin Asakawa <asakawa@ieee.org>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAcSQ_GPV93E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "outputId": "effd9610-1ab5-4a02-ff75-5c3e623557db"
      },
      "source": [
        "# 2020年8月11日現在，以下のコマンドを実行してランタイムを再起動しなければ，このノートブックは動作しません。\n",
        "# 理由は torchtext.datasets のバージョンが 0.3.0 であれば text_classification が定義されていないからです\n",
        "# 以下のコマンドを用いて upgrade すると 0.7.0 にバージョンが更新され，動作するようになります。\n",
        "!pip install --upgrade torchtext"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchtext\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b9/f9/224b3893ab11d83d47fde357a7dcc75f00ba219f34f3d15e06fe4cb62e05/torchtext-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (4.5MB)\n",
            "\u001b[K     |████████████████████████████████| 4.5MB 13.6MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from torchtext) (2.23.0)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 19.4MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: torch in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.6.0+cu101)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2020.6.20)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from torch->torchtext) (0.16.0)\n",
            "Installing collected packages: sentencepiece, torchtext\n",
            "  Found existing installation: torchtext 0.3.1\n",
            "    Uninstalling torchtext-0.3.1:\n",
            "      Successfully uninstalled torchtext-0.3.1\n",
            "Successfully installed sentencepiece-0.1.91 torchtext-0.7.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torchtext"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nD5EXvvuVpjh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from https://github.com/dmlc/xgboost/issues/1715\n",
        "import os\n",
        "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3N34FPtujcuo",
        "colab": {}
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BBH7YQ3ijcu0"
      },
      "source": [
        "## TorchText によるテキスト分類\n",
        "<!--\n",
        "## Text Classification with TorchText\n",
        "-->\n",
        "\n",
        "<!--This tutorial shows how to use the text classification datasets in ``torchtext``, including-->\n",
        "\n",
        "このチュートリアルでは、 ``torchtext`` のテキスト分類データセットの使い方を説明します。\n",
        "\n",
        "- AG_NEWS\n",
        "- SogouNews\n",
        "- DBpedia\n",
        "- YelpReviewPolarity\n",
        "- YelpReviewFull\n",
        "- YahooAnswers\n",
        "- AmazonReviewPolarity\n",
        "- AmazonReviewFull\n",
        "\n",
        "<!--\n",
        "This example shows how to train a supervised learning algorithm for classification using one of these ``TextClassification`` datasets.\n",
        "-->\n",
        "ここでは 上記 ``TextClassification`` データセットを用いて，教師付き学習アルゴリズムによるテキスト分類でどのように訓練するかを示しています。\n",
        "\n",
        "## ngrams によるデータの読み込み\n",
        "<!--\n",
        "## Load data with ngrams\n",
        "-->\n",
        "\n",
        "<!--A bag of ngrams feature is applied to capture some partial information about the local word order. \n",
        "In practice, bi-gram or tri-gram are applied to provide more benefits as word groups than only one word. An example:-->\n",
        "\n",
        "局所的な語順についての部分的な情報を捕捉するため，ngrams 袋の特徴が適用されます。\n",
        "(訳注: Bag of Words 単語袋とは，単語を語順に関わらず詰め込む袋に喩えて Bag of Words と言う)\n",
        "実際には 1 つの単語だけよりも多くの利点を単語群として提供するため，バイグラム bigram (2-gram) または トライグラム (3-gram) が適用されます。\n",
        "例としては\n",
        "\n",
        "<!--\n",
        "```\n",
        "\"load data with ngrams\"\n",
        "Bi-grams results: \"load data\", \"data with\", \"with ngrams\"\n",
        "Tri-grams results: \"load data with\", \"data with ngrams\"\n",
        "```\n",
        "-->\n",
        "\n",
        "```\n",
        "\"load data with ngrams\"\n",
        "Bi-grams 結果: \"load data\", \"data with\", \"with ngrams\"\n",
        "Tri-grams 結果: \"load data with\", \"data with ngrams\"\n",
        "```\n",
        "<!--\n",
        "``TextClassification`` Dataset supports the ngrams method. \n",
        "By setting ngrams to 2, the example text in the dataset will be a list of single words plus bi-grams string.\n",
        "-->\n",
        "\n",
        "``TextClassification`` データセットは ngrams メソッドをサポートしています。\n",
        "ngrams を 2 に設定すると，データセットの例文は，単語のリスト と 2-grams の文字列になります。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RARIsp3yjcu1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "6cd7539a-acb0-4491-beb4-71ce01cacb84"
      },
      "source": [
        "import torch\n",
        "import torchtext\n",
        "from torchtext.datasets import text_classification\n",
        "NGRAMS = 2\n",
        "import os\n",
        "if not os.path.isdir('./.data'):\n",
        "    os.mkdir('./.data')\n",
        "\n",
        "train_dataset, test_dataset = text_classification.DATASETS['AG_NEWS'](\n",
        "    root='./.data', ngrams=NGRAMS, vocab=None)\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ag_news_csv.tar.gz: 11.8MB [00:00, 22.6MB/s]\n",
            "120000lines [00:07, 16086.20lines/s]\n",
            "120000lines [00:16, 7314.09lines/s]\n",
            "7600lines [00:01, 7524.96lines/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5ikNOK9Vpj0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "71b77bd6-a961-44c0-a2cf-e6c97c8694e6"
      },
      "source": [
        "torchtext.__version__"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'0.7.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uc5b2WwCjcu4"
      },
      "source": [
        "## モデルの定義\n",
        "<!--\n",
        "## Define the model\n",
        "-->\n",
        "\n",
        "<!--\n",
        "The model is composed of the\n",
        "[EmbeddingBag](https://pytorch.org/docs/stable/nn.html?highlight=embeddingbag#torch.nn.EmbeddingBag) layer and the linear layer (see the figure below). \n",
        "-->\n",
        "\n",
        "モデルは，[埋め込み袋 EmbeddingBag](https://pytorch.org/docs/stable/nn.html?highlight=embeddingbag#torch.nn.EmbeddingBag) 層と線形層から構成されています。\n",
        "下図を参照してください。\n",
        "\n",
        "<!--\n",
        "``nn.EmbeddingBag`` computes the mean value of a “bag” of embeddings. The text entries here have different lengths. \n",
        "``nn.EmbeddingBag`` requires no padding here since the text lengths are saved in offsets.\n",
        "-->\n",
        "\n",
        "``nn.EmbeddingBag`` は埋め込み「袋」の平均値を計算します。ここでのテキスト項目の長さはそれぞれ異なります。\n",
        "``nn.EmbeddingBag`` は埋め込み (padding) を必要としません。テキストの長さはオフセット（ズレ）で保存されからです。\n",
        "\n",
        "<!--\n",
        "Additionally, since ``nn.EmbeddingBag`` accumulates the average across the embeddings on the fly, ``nn.EmbeddingBag`` can enhance the performance and memory efficiency to process a sequence of tensors.\n",
        "-->\n",
        "\n",
        "さらに ``nn.EmbeddingBag`` は，その場でその都度埋め込み全体の平均値を蓄積します。\n",
        "従って ``nn.EmbeddingsBag`` はテンソル系列を処理する性能とメモリ効率が向上します。\n",
        "\n",
        "<!--\n",
        "![](https://github.com/pytorch/tutorials/blob/gh-pages/_downloads/_static/img/text_sentiment_ngrams_model.png?raw=1)\n",
        "-->\n",
        "\n",
        "<img src=\"https://pytorch.org/tutorials/_images/text_sentiment_ngrams_model.png\">\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bqDb2PMrjcu5",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "class TextSentiment(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_class):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
        "        self.fc = nn.Linear(embed_dim, num_class)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.5\n",
        "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.bias.data.zero_()\n",
        "\n",
        "    def forward(self, text, offsets):\n",
        "        embedded = self.embedding(text, offsets)\n",
        "        return self.fc(embedded)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gurBAngujcvA"
      },
      "source": [
        "## 事例の初期化\n",
        "<!--\n",
        "## Initiate an instance\n",
        "-->\n",
        "\n",
        "<!--The AG_NEWS dataset has four labels and therefore the number of classes is four.-->\n",
        "AG_NEWS データセット は 4 つのラベルを持っており，クラス数は 4つです。\n",
        "\n",
        "```\n",
        "1 : World 世界\n",
        "2 : Sports スポーツ\n",
        "3 : Business ビジネス\n",
        "4 : Sci/Tec 科学工学\n",
        "```\n",
        "\n",
        "<!--\n",
        "The vocab size is equal to the length of vocab (including single word and ngrams). \n",
        "The number of classes is equal to the number of labels, which is four in AG_NEWS case.\n",
        "-->\n",
        "語彙サイズは，語彙の長さ （語彙長）に等しくなります(単語と ngramsを含みます）\n",
        "クラス数はラベル数に等しく，AG_NEWS の場合は 4 です。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wLICqX2FjcvB",
        "colab": {}
      },
      "source": [
        "VOCAB_SIZE = len(train_dataset.get_vocab())\n",
        "EMBED_DIM = 32\n",
        "NUN_CLASS = len(train_dataset.get_labels())\n",
        "model = TextSentiment(VOCAB_SIZE, EMBED_DIM, NUN_CLASS).to(device)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "z-LXKADMjcvL"
      },
      "source": [
        "## バッチ生成に用いる関数\n",
        "<!--\n",
        "## Functions used to generate batch\n",
        "-->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MuknKQGtjcvL"
      },
      "source": [
        "<!--\n",
        "Since the text entries have different lengths, a custom function generate_batch() is used to generate data batches and offsets. \n",
        "The function is passed to ``collate_fn`` in ``torch.utils.data.DataLoader``.\n",
        "The input to ``collate_fn`` is a list of tensors with the size of batch_size, and the ``collate_fn`` function packs them into a mini-batch. \n",
        "Pay attention here and make sure that ``collate_fn`` is declared as a top level def. \n",
        "This ensures that the function is available in each worker.\n",
        "-->\n",
        "\n",
        "テキストエントリの長さはそれぞれ異なるので，データのバッチとオフセットを生成するためにカスタム関数 `generate_batch()` を用います。\n",
        "この関数は ``torch.utils.data.DataLoader``  の ``collate_fn`` に渡されます。\n",
        "``collate_fn`` への入力は `batch_size` サイズのテンソルリストです。 ``collate_fn``関数はそれらをミニバッチにまとめます。\n",
        "ここで注意してほしいのは，``collate_fn`` がトップレベルの関数 (def) として宣言されていることです。\n",
        "これにより，各ワーカーでこの関数が利用できるようになります。\n",
        "\n",
        "<!--\n",
        "The text entries in the original data batch input are packed into a list and concatenated as a single tensor as the input of ``nn.EmbeddingBag``.\n",
        "The offsets is a tensor of delimiters to represent the beginning index of the individual sequence in the text tensor. \n",
        "Label is a tensor saving the labels of individual text entries.\n",
        "-->\n",
        "\n",
        "元のデータ一括入力のテキストエントリはリストに詰められ ``nn.EmbeddingBag`` の入力として 1 つのテンソルとして連結されます。\n",
        "オフセット(ずれ) は，テキストテンソル内の個々の系列の開始インデックスを表す分割子（デリミタ） のテンソルです。\n",
        "ラベル (Label) は，個々のテキスト項目のラベルを保存したテンソルです。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "N2GduZNujcvM",
        "colab": {}
      },
      "source": [
        "def generate_batch(batch):\n",
        "    label = torch.tensor([entry[0] for entry in batch])\n",
        "    text = [entry[1] for entry in batch]\n",
        "    offsets = [0] + [len(entry) for entry in text]\n",
        "    # torch.Tensor.cumsum returns the cumulative sum\n",
        "    # of elements in the dimension dim.\n",
        "    # torch.Tensor([1.0, 2.0, 3.0]).cumsum(dim=0)\n",
        "\n",
        "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
        "    text = torch.cat(text)\n",
        "    return text, offsets, label"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_e_fDPp2jcvQ"
      },
      "source": [
        "## モデルの訓練と結果の評価のための関数の定義\n",
        "<!--\n",
        "## Define functions to train the model and evaluate results.\n",
        "-->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GLNzY6QMjcvR"
      },
      "source": [
        "<!--\n",
        "[torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader) is recommended for PyTorch users, and it makes data loading in parallel easily (a tutorial is [here](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html). \n",
        "We use ``DataLoader`` here to load AG_NEWS datasets and send it to the model for training/validation.\n",
        "-->\n",
        "\n",
        "[torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader) は PyTorch ユーザにおすすめのツールです。\n",
        "これにより，データの読み込みを簡単に並列して行うことができます。チュートリアルは[こちら](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html)。\n",
        "ここでは AG_NEWS データセットをロードしてモデルに送り，訓練や検証を行うために ``DataLoader`` を使用します。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BQhNMMpNjcvR",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def train_func(sub_train_):\n",
        "\n",
        "    # Train the model\n",
        "    train_loss = 0\n",
        "    train_acc = 0\n",
        "    data = DataLoader(sub_train_, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                      collate_fn=generate_batch)\n",
        "    for i, (text, offsets, cls) in enumerate(data):\n",
        "        optimizer.zero_grad()\n",
        "        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
        "        output = model(text, offsets)\n",
        "        loss = criterion(output, cls)\n",
        "        train_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_acc += (output.argmax(1) == cls).sum().item()\n",
        "\n",
        "    # Adjust the learning rate\n",
        "    scheduler.step()\n",
        "\n",
        "    return train_loss / len(sub_train_), train_acc / len(sub_train_)\n",
        "\n",
        "def test(data_):\n",
        "    loss = 0\n",
        "    acc = 0\n",
        "    data = DataLoader(data_, batch_size=BATCH_SIZE, collate_fn=generate_batch)\n",
        "    for text, offsets, cls in data:\n",
        "        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
        "        with torch.no_grad():\n",
        "            output = model(text, offsets)\n",
        "            loss = criterion(output, cls)\n",
        "            loss += loss.item()\n",
        "            acc += (output.argmax(1) == cls).sum().item()\n",
        "\n",
        "    return loss / len(data_), acc / len(data_)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7o7QmeTJjcvY"
      },
      "source": [
        "## データセットの分割とモデルの実行\n",
        "<!--\n",
        "## Split the dataset and run the model\n",
        "-->\n",
        "\n",
        "<!--\n",
        "Since the original AG_NEWS has no valid dataset, we split the training dataset into train/valid sets with a split ratio of 0.95 (train) and\n",
        "0.05 (valid). \n",
        "Here we use [torch.utils.data.dataset.random_split](https://pytorch.org/docs/stable/data.html?highlight=random_split#torch.utils.data.random_split) function in PyTorch core library.\n",
        "-->\n",
        "\n",
        "元の AG_NEWS には検証データセットがないため，学習データセットを 分割率 0.95 (学習データ) で学習/有効 データセットに分割します 0.05 は検証データセット。\n",
        "ここでは PyTorch コアライブラリ [torch.utils.data.dataset.random_split](https://pytorch.org/docs/stable/data.html?highlight=random_split#torch.utils.data.random_split) 関数を使用しています。\n",
        "\n",
        "<!--\n",
        "[CrossEntropyLoss](https://pytorch.org/docs/stable/nn.html?highlight=crossentropyloss#torch.nn.CrossEntropyLoss) criterion combines nn.LogSoftmax() and nn.NLLLoss() in a single class.\n",
        "It is useful when training a classification problem with C classes. \n",
        "[SGD](https://pytorch.org/docs/stable/_modules/torch/optim/sgd.html) implements stochastic gradient descent method as optimizer. \n",
        "The initial learning rate is set to 4.0.\n",
        "[StepLR](https://pytorch.org/docs/master/_modules/torch/optim/lr_scheduler.html#StepLR) is used here to adjust the learning rate through epochs. \n",
        "-->\n",
        "\n",
        "[交差エントロピー損失 CrossEntropyLoss](https://pytorch.org/docs/stable/nn.html?highlight=crossentropyloss#torch.nn.CrossEntropyLoss) 基準は `nn.LogSoftmax()` (訳注: 対数ソフトマックス) と `nn.NLLLoss()` (訳注: 負の対数尤度) を一つのクラスにまとめたものです。\n",
        "C クラスを用いた分類問題を学習する際に便利です。\n",
        "[確率的勾配降下法 SGD](https://pytorch.org/docs/stable/_modules/torch/optim/sgd.html) はオプティマイザとして確率的勾配降下法を実装しています。\n",
        "初期学習率は 4.0 に設定されています。\n",
        "ここでは [StepLR](https://pytorch.org/docs/master/_modules/torch/optim/lr_scheduler.html#StepLR) を用いてエポック単位で学習率を調整しています。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5CRTSvonjcva",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "e8fbb6dc-4024-4ef7-f6bd-0de58713a511"
      },
      "source": [
        "import time\n",
        "from torch.utils.data.dataset import random_split\n",
        "N_EPOCHS = 5\n",
        "min_valid_loss = float('inf')\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=4.0)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)\n",
        "\n",
        "train_len = int(len(train_dataset) * 0.95)\n",
        "sub_train_, sub_valid_ = \\\n",
        "    random_split(train_dataset, [train_len, len(train_dataset) - train_len])\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    train_loss, train_acc = train_func(sub_train_)\n",
        "    valid_loss, valid_acc = test(sub_valid_)\n",
        "\n",
        "    secs = int(time.time() - start_time)\n",
        "    mins = secs / 60\n",
        "    secs = secs % 60\n",
        "\n",
        "    print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n",
        "    print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n",
        "    print(f'\\tLoss: {valid_loss:.4f}(valid)\\t|\\tAcc: {valid_acc * 100:.1f}%(valid)')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1  | time in 0 minutes, 8 seconds\n",
            "\tLoss: 0.0261(train)\t|\tAcc: 84.7%(train)\n",
            "\tLoss: 0.0000(valid)\t|\tAcc: 88.7%(valid)\n",
            "Epoch: 2  | time in 0 minutes, 8 seconds\n",
            "\tLoss: 0.0120(train)\t|\tAcc: 93.6%(train)\n",
            "\tLoss: 0.0000(valid)\t|\tAcc: 91.5%(valid)\n",
            "Epoch: 3  | time in 0 minutes, 9 seconds\n",
            "\tLoss: 0.0068(train)\t|\tAcc: 96.5%(train)\n",
            "\tLoss: 0.0001(valid)\t|\tAcc: 91.6%(valid)\n",
            "Epoch: 4  | time in 0 minutes, 8 seconds\n",
            "\tLoss: 0.0039(train)\t|\tAcc: 98.1%(train)\n",
            "\tLoss: 0.0000(valid)\t|\tAcc: 91.2%(valid)\n",
            "Epoch: 5  | time in 0 minutes, 8 seconds\n",
            "\tLoss: 0.0023(train)\t|\tAcc: 99.0%(train)\n",
            "\tLoss: 0.0001(valid)\t|\tAcc: 91.7%(valid)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "62-Lz0S1jcvf"
      },
      "source": [
        "<!--\n",
        "Running the model on GPU with the following information:\n",
        "-->\n",
        "GPU を用いてモデルを実行すると以下のよう情報を得ます\n",
        "\n",
        "```\n",
        "Epoch: 1 | time in 0 minutes, 11 seconds\n",
        "       Loss: 0.0263(train)     |       Acc: 84.5%(train)\n",
        "       Loss: 0.0001(valid)     |       Acc: 89.0%(valid)\n",
        "\n",
        "Epoch: 2 | time in 0 minutes, 10 seconds\n",
        "       Loss: 0.0119(train)     |       Acc: 93.6%(train)\n",
        "       Loss: 0.0000(valid)     |       Acc: 89.6%(valid)\n",
        "\n",
        "Epoch: 3 | time in 0 minutes, 9 seconds\n",
        "       Loss: 0.0069(train)     |       Acc: 96.4%(train)\n",
        "       Loss: 0.0000(valid)     |       Acc: 90.5%(valid)\n",
        "\n",
        "Epoch: 4 | time in 0 minutes, 11 seconds\n",
        "       Loss: 0.0038(train)     |       Acc: 98.2%(train)\n",
        "       Loss: 0.0000(valid)     |       Acc: 90.4%(valid)\n",
        "\n",
        "\n",
        "Epoch: 5 | time in 0 minutes, 11 seconds\n",
        "       Loss: 0.0022(train)     |       Acc: 99.0%(train)\n",
        "       Loss: 0.0000(valid)     |       Acc: 91.0%(valid)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1nNGPxHCjcvg"
      },
      "source": [
        "## テストデータを用いたモデルの評価\n",
        "<!--\n",
        "## Evaluate the model with test dataset\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WGBzwperjcvg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "648f7270-63ba-451d-c5d1-46eaf3745f97"
      },
      "source": [
        "print('Checking the results of test dataset...')\n",
        "test_loss, test_acc = test(test_dataset)\n",
        "print(f'\\tLoss: {test_loss:.4f}(test)\\t|\\tAcc: {test_acc * 100:.1f}%(test)')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Checking the results of test dataset...\n",
            "\tLoss: 0.0002(test)\t|\tAcc: 90.6%(test)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "M4wD9vq8jcvk"
      },
      "source": [
        "テストデータセットの結果をチェック:\n",
        "\n",
        "<!--\n",
        "Checking the results of test dataset…\n",
        "-->\n",
        "\n",
        "```\n",
        "       Loss: 0.0237(test)      |       Acc: 90.5%(test)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Vp1bTWv_jcvk"
      },
      "source": [
        "## ランダムニューズでのテスト\n",
        "<!--\n",
        "## Test on a random news\n",
        "-->\n",
        "\n",
        "<!--\n",
        "Use the best model so far and test a golf news. The label information is available [here](https://pytorch.org/text/datasets.html?highlight=ag_news#torchtext.datasets.AG_NEWS).\n",
        "-->\n",
        "\n",
        "今までの最良モデルを使ってゴルフニュースを試してみてください。\n",
        "ラベル情報は[こちら](https://pytorch.org/text/datasets.html?highlight=ag_news#torchtext.datasets.AG_NEWS)。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7XAfishYjcvl",
        "colab": {},
        "outputId": "4caee229-a759-48f0-fbae-f1ccb3f72d45"
      },
      "source": [
        "import re\n",
        "from torchtext.data.utils import ngrams_iterator\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "ag_news_label = {1 : \"World\",\n",
        "                 2 : \"Sports\",\n",
        "                 3 : \"Business\",\n",
        "                 4 : \"Sci/Tec\"}\n",
        "\n",
        "def predict(text, model, vocab, ngrams):\n",
        "    tokenizer = get_tokenizer(\"basic_english\")\n",
        "    with torch.no_grad():\n",
        "        text = torch.tensor([vocab[token]\n",
        "                            for token in ngrams_iterator(tokenizer(text), ngrams)])\n",
        "        output = model(text, torch.tensor([0]))\n",
        "        return output.argmax(1).item() + 1\n",
        "\n",
        "ex_text_str = \"MEMPHIS, Tenn. – Four days ago, Jon Rahm was \\\n",
        "    enduring the season’s worst weather conditions on Sunday at The \\\n",
        "    Open on his way to a closing 75 at Royal Portrush, which \\\n",
        "    considering the wind and the rain was a respectable showing. \\\n",
        "    Thursday’s first round at the WGC-FedEx St. Jude Invitational \\\n",
        "    was another story. With temperatures in the mid-80s and hardly any \\\n",
        "    wind, the Spaniard was 13 strokes better in a flawless round. \\\n",
        "    Thanks to his best putting performance on the PGA Tour, Rahm \\\n",
        "    finished with an 8-under 62 for a three-stroke lead, which \\\n",
        "    was even more impressive considering he’d never played the \\\n",
        "    front nine at TPC Southwind.\"\n",
        "\n",
        "vocab = train_dataset.get_vocab()\n",
        "model = model.to(\"cpu\")\n",
        "\n",
        "print(\"This is a %s news\" %ag_news_label[predict(ex_text_str, model, vocab, 2)])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is a Sports news\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kyzCwYhHjcvq"
      },
      "source": [
        "これはスポーツニュースです\n",
        "<!--This is a Sports news-->\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oBj4aVZLjcvr"
      },
      "source": [
        "<!--\n",
        "You can find the code examples displayed in this note [here](https://github.com/pytorch/text/tree/master/examples/text_classification) \n",
        "-->\n",
        "\n",
        "このノートのコードは [こちら](https://github.com/pytorch/text/tree/master/examples/text_classification) です。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67F5dbFOVpkp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}