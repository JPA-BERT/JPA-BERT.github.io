{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "2020-0726transformer_tutorial.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JPA-BERT/jpa-bert.github.io/blob/master/notebooks/2020_0726transformer_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clu1nPAXYW9p",
        "colab_type": "text"
      },
      "source": [
        "# PyTorch チュートリアルにある トランスフォーマーチュートリアル\n",
        "- source: https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
        "- colab source: https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/dca13261bbb4e9809d1a3aa521d22dd7/transformer_tutorial.ipynb\n",
        "- date: 2020-0726\n",
        "- author: 浅川伸一 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CoPvqm6iSJBZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HY02Vn8MSJBe",
        "colab_type": "text"
      },
      "source": [
        "# nn.Transformer と TorchText を用いた Seq2seq モデル\n",
        "<!--\n",
        "# Sequence-to-Sequence Modeling with nn.Transformer and TorchText\n",
        "-->\n",
        "\n",
        "本チュートリアルでは，[`nn.Transformer`](https://pytorch.org/docs/master/nn.html?highlight=nn%20transformer#torch.nn.Transformer) モジュールを用いた，seq2seq モデルの訓練を取り上げます。\n",
        "PyTorch 1.2 のリリースには  [`Attention is All You Need`](https://arxiv.org/pdf/1706.03762.pdf) 論文に基づいた標準的なトランスフォーマモジュールが実装されています。\n",
        "このモデルは，多くの seq2seq 問題に対して，より並列化が可能であり，品質も優れていることが示されています。\n",
        "nn.Transformer``モジュールは、入力と出力の間のグローバルな依存関係を描画するための注意機構 (最近 [nn.MultiheadAttention`](https://pytorch.org/docs/master/nn.html?highlight=multiheadattention#torch.nn.MultiheadAttention) として実装された別のモジュール) に完全に依存しています。\n",
        "このチュートリアルの [nn.TransformerEncoder`](https://pytorch.org/docs/master/nn.html?highlight=nn%20transformerencoder#torch.nn.TransformerEncoder) のような単一のコンポーネントを簡単に適応させることができるように ``nn.Transformer` モジュールは高度にモジュール化されています。\n",
        "\n",
        "<!--\n",
        "This is a tutorial on how to train a sequence-to-sequence model that uses the [`nn.Transformer`](https://pytorch.org/docs/master/nn.html?highlight=nn%20transformer#torch.nn.Transformer) module.\n",
        "\n",
        "PyTorch 1.2 release includes a standard transformer module based on the paper [`Attention is All You Need`](https://arxiv.org/pdf/1706.03762.pdf). \n",
        "The transformer model has been proved to be superior in quality for many sequence-to-sequence problems while being more parallelizable. \n",
        "The ``nn.Transformer`` module relies entirely on an attention mechanism (another module recently implemented as [`nn.MultiheadAttention`](https://pytorch.org/docs/master/nn.html?highlight=multiheadattention#torch.nn.MultiheadAttention) to draw global dependencies between input and output. \n",
        "The ``nn.Transformer`` module is now highly modularized such that a single component (like [`nn.TransformerEncoder`](https://pytorch.org/docs/master/nn.html?highlight=nn%20transformerencoder#torch.nn.TransformerEncoder) in this tutorial) can be easily adapted/composed.\n",
        "-->\n",
        "<!--![](../_static/img/transformer_architecture.jpg)-->\n",
        "![](https://pytorch.org/tutorials/_images/transformer_architecture.jpg)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIb7Iu_LSJBf",
        "colab_type": "text"
      },
      "source": [
        "# モデルの定義\n",
        "\n",
        "<!--\n",
        "Define the model\n",
        "----------------\n",
        "-->\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymoRUhkvSJBf",
        "colab_type": "text"
      },
      "source": [
        "<!--\n",
        "In this tutorial, we train ``nn.TransformerEncoder`` model on a language modeling task. \n",
        "The language modeling task is to assign a probability for the likelihood of a given word (or a sequence of words) to follow a sequence of words. \n",
        "A sequence of tokens are passed to the embedding layer first, followed by a positional encoding layer to account for the order of the word (see the next paragraph for more details). \n",
        "The ``nn.TransformerEncoder`` consists of multiple layers of [`nn.TransformerEncoderLayer`](https://pytorch.org/docs/master/nn.html?highlight=transformerencoderlayer#torch.nn.TransformerEncoderLayer). \n",
        "Along with the input sequence, a square attention mask is required because the self-attention layers in ``nn.TransformerEncoder`` are only allowed to attend the earlier positions in the sequence. \n",
        "For the language modeling task, any tokens on the future positions should be masked. \n",
        "To have the actual words, the output of ``nn.TransformerEncoder`` model is sent to the final Linear layer, which is followed by a log-Softmax function.\n",
        "-->\n",
        "\n",
        "このチュートリアルでは，言語モデル課題で ``nn.TransformerEncoder`` モデルを学習します。\n",
        "言語モデル課題とは，与えられた単語 (または単語系列) が単語系列に従う確率を割り当てる課題です。\n",
        "一連のトークンは最初に埋め込み層に渡され，次に単語の順番を考慮した位置エンコーディング層が続きます (詳細は次段落を参照)。\n",
        "nn.TransformerEncoder``は、[nn.TransformerEncoderLayer`] (https://pytorch.org/docs/master/nn.html?highlight=transformerencoderlayer#torch.nn.TransformerEncoderLayer) の複数の層から構成されています。\n",
        "入力系列とともに、 ``nn.TransformerEncoder`` の自己注意層は系列の初頭位置にしか注意を払うことができないため，四角い注意マスクが必要となります。(訳注:上図マルチヘッドアテンションのこと)\n",
        "言語モデル課題では，将来の位置のトークンはマスクされるべきです。\n",
        "実際の単語を得るために ``nn.TransformerEncoder`` モデルの出力は最終的な線形層に送られ，\n",
        "だらに log-Softmax 関数が続きます。\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHO_vTn4SJBg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "\n",
        "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "        self.model_type = 'Transformer'\n",
        "        self.src_mask = None\n",
        "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
        "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.encoder = nn.Embedding(ntoken, ninp)\n",
        "        self.ninp = ninp\n",
        "        self.decoder = nn.Linear(ninp, ntoken)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def _generate_square_subsequent_mask(self, sz):\n",
        "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "        return mask\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, src):\n",
        "        if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
        "            device = src.device\n",
        "            mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
        "            self.src_mask = mask\n",
        "\n",
        "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
        "        src = self.pos_encoder(src)\n",
        "        output = self.transformer_encoder(src, self.src_mask)\n",
        "        output = self.decoder(output)\n",
        "        return output"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xFRU9Z0SJBj",
        "colab_type": "text"
      },
      "source": [
        "<!--\n",
        "``PositionalEncoding`` module injects some information about the relative or absolute position of the tokens in the sequence. \n",
        "The positional encodings have the same dimension as the embeddings so that the two can be summed. Here, we use ``sine`` and ``cosine`` functions of different frequencies.\n",
        "-->\n",
        "\n",
        "``PositionalEncoding`` モジュールは系列中のトークンの相対位置や絶対位置に関する情報を注入します。\n",
        "位置符号化器は埋め込み層と同じ次元を持ち，両者を合算することができます。\n",
        "ここでは，異なる周波数の ``sine``` と ``cosine``` 関数を利用しています。\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsNPMA9RSJBk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGkXRVg-SJBn",
        "colab_type": "text"
      },
      "source": [
        "# データのロードとバッチ化\n",
        "\n",
        "<!--\n",
        "Load and batch data\n",
        "-------------------\n",
        "-->\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeveZKlbSJBn",
        "colab_type": "text"
      },
      "source": [
        "<!--The training process uses Wikitext-2 dataset from ``torchtext``. \n",
        "The vocab object is built based on the train dataset and is used to numericalize tokens into tensors. \n",
        "Starting from sequential data, the ``batchify()`` function arranges the dataset into columns, trimming off any tokens remaining after the data has been divided into batches of size ``batch_size``.\n",
        "For instance, with the alphabet as the sequence (total length of 26) and a batch size of 4, we would divide the alphabet into 4 sequences of length 6:-->\n",
        "\n",
        "訓練には ``torchtext`` の Wikitext-2 データセットを使用します。\n",
        "Vocab オブジェクトは訓練データセットに元づいて構築され，トークンをテンソルへと数値化するために使用されます。\n",
        "連続したデータから ``batchify()`` 関数を使ってデータを列に配置し， ``batch_size`` の大きさのバッチに分割した後に残ったトークンを切り取ります。\n",
        "例えば，アルファベットをシーケンス（全長 26) とし，（ミニ）バッチサイズを 4 とした場合，\n",
        "アルファベットを長さ 6 の 4 系列に分割します:\n",
        "\n",
        "\\begin{align}\\begin{bmatrix}\n",
        "  \\text{A} & \\text{B} & \\text{C} & \\ldots & \\text{X} & \\text{Y} & \\text{Z}\n",
        "  \\end{bmatrix}\n",
        "  \\Rightarrow\n",
        "  \\begin{bmatrix}\n",
        "  \\begin{bmatrix}\\text{A} \\\\ \\text{B} \\\\ \\text{C} \\\\ \\text{D} \\\\ \\text{E} \\\\ \\text{F}\\end{bmatrix} &\n",
        "  \\begin{bmatrix}\\text{G} \\\\ \\text{H} \\\\ \\text{I} \\\\ \\text{J} \\\\ \\text{K} \\\\ \\text{L}\\end{bmatrix} &\n",
        "  \\begin{bmatrix}\\text{M} \\\\ \\text{N} \\\\ \\text{O} \\\\ \\text{P} \\\\ \\text{Q} \\\\ \\text{R}\\end{bmatrix} &\n",
        "  \\begin{bmatrix}\\text{S} \\\\ \\text{T} \\\\ \\text{U} \\\\ \\text{V} \\\\ \\text{W} \\\\ \\text{X}\\end{bmatrix}\n",
        "  \\end{bmatrix}\\end{align}\n",
        "\n",
        "<!--These columns are treated as independent by the model, which means that the dependence of ``G`` and ``F`` can not be learned, but allows more efficient batch processing.-->\n",
        "\n",
        "これらの列はモデルによって独立したものとして扱われ， ``G`` と ``F`` の依存性を学習することはできません。\n",
        "より効率的なバッチ処理が可能になります。\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "disMTe5yUpvG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 下のセルでは 3 行目の tokenizer の指定 'basic_english' でエラーになる\n",
        "# 'basic_english' を次のいずれかに変更する必要がある:\n",
        "# 'moses', 'spacy', revtok', 'subword' \n",
        "#!pip install sacremoses  # for moses tokenizer\n",
        "#!pip install revtok  # for revtok tokenizer"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kpp-7xucSJBo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "b2685fec-3775-455b-9b85-b156553a7a55"
      },
      "source": [
        "import torchtext\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "#TEXT = torchtext.data.Field(tokenize=get_tokenizer(\"basic_english\"),\n",
        "#TEXT = torchtext.data.Field(tokenize=get_tokenizer(\"moses\"),\n",
        "TEXT = torchtext.data.Field(tokenize=get_tokenizer(\"spacy\"),\n",
        "#TEXT = torchtext.data.Field(tokenize=get_tokenizer(\"revtok\"),\n",
        "#TEXT = torchtext.data.Field(tokenize=get_tokenizer(\"subword\"),\n",
        "                            init_token='<sos>',\n",
        "                            eos_token='<eos>',\n",
        "                            lower=True)\n",
        "train_txt, val_txt, test_txt = torchtext.datasets.WikiText2.splits(TEXT)\n",
        "TEXT.build_vocab(train_txt)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def batchify(data, bsz):\n",
        "    data = TEXT.numericalize([data.examples[0].text])\n",
        "    # Divide the dataset into bsz parts.\n",
        "    nbatch = data.size(0) // bsz\n",
        "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
        "    data = data.narrow(0, 0, nbatch * bsz)\n",
        "    # Evenly divide the data across the bsz batches.\n",
        "    data = data.view(bsz, -1).t().contiguous()\n",
        "    return data.to(device)\n",
        "\n",
        "batch_size = 20\n",
        "eval_batch_size = 10\n",
        "train_data = batchify(train_txt, batch_size)\n",
        "val_data = batchify(val_txt, eval_batch_size)\n",
        "test_data = batchify(test_txt, eval_batch_size)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading wikitext-2-v1.zip\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "wikitext-2-v1.zip: 100%|██████████| 4.48M/4.48M [00:01<00:00, 3.10MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "extracting\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3TvR8KCSJBq",
        "colab_type": "text"
      },
      "source": [
        "### 入出力系列を生成する関数\n",
        "<!--\n",
        "### Functions to generate input and target sequence\n",
        "-->\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1ONg58_SJBr",
        "colab_type": "text"
      },
      "source": [
        "<!--\n",
        "``get_batch()`` function generates the input and target sequence for the transformer model. \n",
        "It subdivides the source data into chunks of length ``bptt``. \n",
        "For the language modeling task, the model needs the following words as ``Target``. \n",
        "For example, with a ``bptt`` value of 2, we’d get the following two Variables for ``i`` = 0:\n",
        "-->\n",
        "\n",
        "関数 ``get_batch()`` はトランスフォーマモデルの入力系列と目標系列を生成します。\n",
        "ソースデータを長さ  ``bptt`` のチャンクに分割します。\n",
        "言語モデル課題では，モデルは 入力系列に後続する  ``Target`` 単語系列を必要とします。\n",
        "例えば  ``bptt`` の値が 2 ではれば， ``i`` = 0 のときには次の 2 つの変数が得られます。\n",
        "\n",
        "<!--![](../_static/img/transformer_input_target.png)-->\n",
        "![](https://pytorch.org/tutorials/_images/transformer_input_target.png)\n",
        "\n",
        "\n",
        "It should be noted that the chunks are along dimension 0, consistent with the ``S`` dimension in the Transformer model. The batch dimension ``N`` is along dimension 1.\n",
        "\n",
        "チャンクは次元 0 (訳注:行数) に沿っており，トランスフォーマーモデルの ``S`` 次元と一致していることに注意してください。\n",
        "バッチ次元 ``N`` は 次元 1 (訳注:列数) に沿っています。\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEQQfkeySJBr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bptt = 35\n",
        "def get_batch(source, i):\n",
        "    seq_len = min(bptt, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "    target = source[i+1:i+1+seq_len].view(-1)\n",
        "    return data, target"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgMEA5v0SJBv",
        "colab_type": "text"
      },
      "source": [
        "## インスタンスの初期化\n",
        "<!--\n",
        "Initiate an instance\n",
        "--------------------\n",
        "-->\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OfH-re-SJBv",
        "colab_type": "text"
      },
      "source": [
        "<!--\n",
        "The model is set up with the hyperparameter below. \n",
        "The vocab size is equal to the length of the vocab object.\n",
        "-->\n",
        "\n",
        "モデルには以下のハイパーパラメータが設定されています。\n",
        "語彙数は vocab オブジェクトの長さに等しくなります。\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uyjNlYCSJBw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ntokens = len(TEXT.vocab.stoi) # 総語彙数\n",
        "emsize = 200 #  埋め込み次元，すなわち埋め込み層のニューロン数\n",
        "nhid = 200 # nn.TransformerEncoder の順行ネットワークの次元数，すなわち中間層のニューロン数\n",
        "nlayers = 2 # nn.TransformerEncoder における nn.TransfomerEncoderLayer の数\n",
        "nhead = 2 # マルチヘッド注意のヘッド数 \n",
        "dropout = 0.2 # ドロップアウト率\n",
        "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leUtTHRaSJBz",
        "colab_type": "text"
      },
      "source": [
        "## モデルの実行\n",
        "\n",
        "<!--\n",
        "Run the model\n",
        "-------------\n",
        "-->\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzW-gREvSJB0",
        "colab_type": "text"
      },
      "source": [
        "[`CrossEntropyLoss`](https://pytorch.org/docs/master/nn.html?highlight=crossentropyloss#torch.nn.CrossEntropyLoss) is applied to track the loss and [`SGD`](https://pytorch.org/docs/master/optim.html?highlight=sgd#torch.optim.SGD) implements stochastic gradient descent method as the optimizer. \n",
        "The initial learning rate is set to 5.0. \n",
        "[`StepLR`](https://pytorch.org/docs/master/optim.html?highlight=steplr#torch.optim.lr_scheduler.StepLR) is applied to adjust the learn rate through epochs. \n",
        "During the training, we use [`nn.utils.clip_grad_norm`](https://pytorch.org/docs/master/nn.html?highlight=nn%20utils%20clip_grad_norm#torch.nn.utils.clip_grad_norm_) function to scale all the gradient together to prevent exploding.\n",
        "\n",
        "\n",
        "損失関数には交差エントロピー損失  [`CrossEntropyLoss`](https://pytorch.org/docs/master/nn.html?highlight=crossentropyloss#torch.nn.CrossEntropyLoss) を，\n",
        "最適化関数には，確率的勾配降下法 [`SGD`](https://pytorch.org/docs/master/optim.html?highlight=sgd#torch.optim.SGD) の実装を用います。\n",
        "初期学習率は 5.0 に設定します。\n",
        "学習率をエポック単位で調整するために [`StepLR`](https://pytorch.org/docs/master/optim.html?highlight=steplr#torch.optim.lr_scheduler.StepLR) を適用します。\n",
        "学習中に勾配クリップ [`nn.utils.clip_grad_norm`](https://pytorch.org/docs/master/nn.html?highlight=nn%20utils%20clip_grad_norm#torch.nn.utils.clip_grad_norm_) 関数を用いて勾配爆発を防ぐために全ての勾配をまとめてスケーリングします。(訳注:リカレントニューラルネットワークにおいては，勾配消失問題と共に購買爆発問題が生じる可能性がある。このため，適当なしきい値を設定して爆発を防ぐことが行われる)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1OGmnNkSJB0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "lr = 5.0 # learning rate\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
        "\n",
        "import time\n",
        "def train():\n",
        "    model.train() # Turn on the train mode\n",
        "    total_loss = 0.\n",
        "    start_time = time.time()\n",
        "    ntokens = len(TEXT.vocab.stoi)\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
        "        data, targets = get_batch(train_data, i)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output.view(-1, ntokens), targets)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        log_interval = 200\n",
        "        if batch % log_interval == 0 and batch > 0:\n",
        "            cur_loss = total_loss / log_interval\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
        "                  'lr {:02.2f} | ms/batch {:5.2f} | '\n",
        "                  'loss {:5.2f} | ppl {:8.2f}'.format(\n",
        "                    epoch, batch, len(train_data) // bptt, scheduler.get_last_lr()[0], # scheduler.get_lr()[0],\n",
        "                    elapsed * 1000 / log_interval,\n",
        "                    cur_loss, math.exp(cur_loss)))\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n",
        "\n",
        "def evaluate(eval_model, data_source):\n",
        "    eval_model.eval() # Turn on the evaluation mode\n",
        "    total_loss = 0.\n",
        "    ntokens = len(TEXT.vocab.stoi)\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, data_source.size(0) - 1, bptt):\n",
        "            data, targets = get_batch(data_source, i)\n",
        "            output = eval_model(data)\n",
        "            output_flat = output.view(-1, ntokens)\n",
        "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
        "    return total_loss / (len(data_source) - 1)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2or7ZuqDSJB3",
        "colab_type": "text"
      },
      "source": [
        "Loop over epochs. Save the model if the validation loss is the best we've seen so far. Adjust the learning rate after each epoch.\n",
        "\n",
        "エポックを繰り返します。\n",
        "検証データの損失値が以前のエポックのときの値よりも良ければ，その最善のモデルを保存します。\n",
        "各エポック終了後に学習率を調整します。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_5AyaECSJB3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "42e3af10-e3ba-4026-82dc-2383764bd040"
      },
      "source": [
        "best_val_loss = float(\"inf\")\n",
        "epochs = 3 # エポック数\n",
        "#epochs = 1 # エポック数\n",
        "best_model = None\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train()\n",
        "    val_loss = evaluate(model, val_data)\n",
        "    print('-' * 89)\n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "          'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                     val_loss, math.exp(val_loss)))\n",
        "    print('-' * 89)\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_model = model\n",
        "\n",
        "    scheduler.step()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| epoch   1 |   200/ 3195 batches | lr 5.00 | ms/batch 37.97 | loss  4.82 | ppl   124.53\n",
            "| epoch   1 |   400/ 3195 batches | lr 5.00 | ms/batch 37.42 | loss  5.05 | ppl   155.81\n",
            "| epoch   1 |   600/ 3195 batches | lr 5.00 | ms/batch 37.34 | loss  4.95 | ppl   141.81\n",
            "| epoch   1 |   800/ 3195 batches | lr 5.00 | ms/batch 37.40 | loss  4.93 | ppl   138.73\n",
            "| epoch   1 |  1000/ 3195 batches | lr 5.00 | ms/batch 37.44 | loss  5.03 | ppl   152.67\n",
            "| epoch   1 |  1200/ 3195 batches | lr 5.00 | ms/batch 37.42 | loss  5.00 | ppl   148.85\n",
            "| epoch   1 |  1400/ 3195 batches | lr 5.00 | ms/batch 37.45 | loss  5.05 | ppl   156.14\n",
            "| epoch   1 |  1600/ 3195 batches | lr 5.00 | ms/batch 37.45 | loss  4.98 | ppl   145.19\n",
            "| epoch   1 |  1800/ 3195 batches | lr 5.00 | ms/batch 37.41 | loss  5.02 | ppl   152.12\n",
            "| epoch   1 |  2000/ 3195 batches | lr 5.00 | ms/batch 37.39 | loss  5.05 | ppl   156.58\n",
            "| epoch   1 |  2200/ 3195 batches | lr 5.00 | ms/batch 37.38 | loss  5.01 | ppl   149.83\n",
            "| epoch   1 |  2400/ 3195 batches | lr 5.00 | ms/batch 37.44 | loss  4.91 | ppl   136.20\n",
            "| epoch   1 |  2600/ 3195 batches | lr 5.00 | ms/batch 37.46 | loss  4.99 | ppl   147.60\n",
            "| epoch   1 |  2800/ 3195 batches | lr 5.00 | ms/batch 37.50 | loss  4.98 | ppl   145.89\n",
            "| epoch   1 |  3000/ 3195 batches | lr 5.00 | ms/batch 37.46 | loss  4.91 | ppl   135.88\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 124.45s | valid loss  4.96 | valid ppl   142.42\n",
            "-----------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WF8q0NA7SJB6",
        "colab_type": "text"
      },
      "source": [
        "## テストデータを用いたモデルの評価\n",
        "\n",
        "テストデータを用いて上で学習させたモデルを評価します。\n",
        "\n",
        "<!--\n",
        "Evaluate the model with the test dataset\n",
        "-------------------------------------\n",
        "\n",
        "Apply the best model to check the result with the test dataset.\n",
        "-->\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTOTkIBISJB7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "c2952eee-007e-4b68-aaa5-cca1511d0664"
      },
      "source": [
        "test_loss = evaluate(best_model, test_data)\n",
        "print('=' * 89)\n",
        "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
        "    test_loss, math.exp(test_loss)))\n",
        "print('=' * 89)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=========================================================================================\n",
            "| End of training | test loss  4.80 | test ppl   121.70\n",
            "=========================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KL2KsqVFoi2t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}