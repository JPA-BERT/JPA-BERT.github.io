---
layout: post
title: 導入
---

* [Googleの検索エンジンに「過去5年で最大の飛躍」。新たな言語処理モデル「BERT」の秘密](https://wired.jp/2019/11/14/google-search-advancing-grade-reading/?fbclid=IwAR0ZyWC5Ll7xwtg0yaEMFv15jk3_BZZkKXf3dTs_jbNr-DVoKSAZUtsKlSo#%E2%80%A6) ([Original](https://www.wired.com/story/google-search-advancing-grade-reading/))
* [Context and Compositionality in Biological and Artificial Neural Systems](https://context-composition.github.io/) in neurips2019
	1. [Tom Mitchell](https://slideslive.com/38922193/context-and-compositionality-in-biological-and-artificial-neural-systems-2) 
	    - 2019年は2018年までの世界とは異なる (45:40 くらいから)
        - (1:01:40くらい) おそらく我々(人類)は今特別な時点にいる。脳が数千年に渡って実行してきた機能を実行する深層学習モデルを持ったからだ。

	![](/assets/2019mitchell-54_20.png)
	![](/assets/2019mitchell_2.png)
	![](/assets/2019mitchell_3.png)
	![](/assets/2019mitchell_4.png)

	2. [Yoshua Bengio](https://slideslive.com/38922080/context-and-compositionality-in-biological-and-artificial-neural-systems-3) (20:20 くらいから) システム 1 とシステム 2 とを結びつける鍵は注意である。


### リンク

* [The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html)
* [Illustrated transformer](http://jalammar.github.io/illustrated-transformer/)
* [The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning](http://jalammar.github.io/illustrated-bert/)
    * [T2T notebook](https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb)
    - [Deconstructing BERT: Distilling 6 Patterns from 100 Million Parameters](https://towardsdatascience.com/deconstructing-bert-distilling-6-patterns-from-100-million-parameters-b49113672f77)
    - [Deconstructing BERT, Part 2: Visualizing the Inner Workings of Attention](https://towardsdatascience.com/deconstructing-bert-part-2-visualizing-the-inner-workings-of-attention-60a16d86b5c1)
        - [注意の視覚化ツール](https://github.com/jessevig/bertviz)
        - [Visualizing Attention in Transformer-Based Language Representation Models](https://arxiv.org/abs/1904.02679), [YouTube](https://www.youtube.com/watch?v=69alFnWcVl4K)
        - [A Multiscale Visualization of Attention in the Transformer Model](https://arxiv.org/abs/1906.05714), [video](https://vimeo.com/340841955)
        - [Analyzing the Structure of Attention in a Transformer Language Model](https://arxiv.org/abs/1906.04284)
- <https://github.com/tensorflow/tensor2tensor>
- <https://github.com/huggingface/pytorch-pretrained-BERT>


### 自然言語処理諸課題における各モデルの性能比較 GLUE leaderboad

- [GLUE](https://gluebenchmark.com/leaderboard/)
- [super GLUE](https://super.gluebenchmark.com/leaderboard/)

